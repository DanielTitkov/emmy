{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import findall as fa\n",
    "import sqlite3\n",
    "import pymorphy2\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Иногда я пишу о книгах, которые произвели на меня впечатление. Писать большие отзывы сейчас не хочется, поэтому в порядке перечисления.\n",
    "\"Атлант расправил плечи\" - за последнее время понравилась больше всего наряду с Довлатовым (но насчет последнего сомнений и не было). Почему-то раньше я думал, что это что-то вроде \"Финансиста\" Драйзера. Так же, видимо, думают и люди, рисующие мемы \"сын маминой подруги расправил плечи\". А на самом деле книга об альтернативной вселенной, где в США наступил социализм. Очень рекомендую.\n",
    "Что до \"Финансиста\" Драйзера, то он надолго отбил у меня желание читать этого автора. Не потому что мне не интересно читать про рынок - наоборот, про рынок интересно. Но всё остальное там скучно, особенно герои. Может быть, так и было задумано, но я это не люблю.\n",
    "Дилогия об Остапе Бендере - начинаются обе книги весело, кончаются обе книги уныло. Не столько с точки зрения событий, сколько с точки зрения того, как трансформируется язык. Поэтому от них остается неприятное ощущение, хотя написаны они ярко, весело и интересно. Впрочем, не пойти на такую сделку вряд ли можно было в условиях, в которых работали авторы.\n",
    "\"Три мушкетера\". Ну, не побоюсь этого слова, такое. Занятно, но не более того - я сейчас даже с трудом вспомнил об этой книжке. Главный интерес книжка представляет с исторической точки зрения. В том числе и потому, что является убедительным доказательством, что во Франции в 17м веке был интернет и портативные телепорты - ну или по крайней мере бесстыдная сценарная магия.\n",
    "Отто Кариус, \"Тигры в грязи\". Язык этой книги совершенно ужасен, может быть, потому что её писал солдат. Но прочитать очень стоит, потому что мало что может быть так ценно, как новая точка зрения на нечто хорошо знакомое!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(s):\n",
    "    rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "    return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpos= ['PRED', 'None', 'PRTS', 'ADJF', 'INFN', \n",
    "         'PRTF', 'NOUN', 'ADVB', 'VERB', 'NPRO', \n",
    "         'NUMR', 'CONJ', 'ADJS', 'PRCL', 'PREP', 'COMP', 'INTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, \n",
    "                     morph=pymorphy2.MorphAnalyzer(), \n",
    "                     pos_types=['ADJF', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP'],\n",
    "                    ):\n",
    "    \n",
    "    #length in chars and words\n",
    "    len_char = len(text)\n",
    "    len_word = len(text.split())\n",
    "    pun = fa('[\\.+,!\\?:-]',text)\n",
    "    n_pun = len(pun)\n",
    "    braсket_list = fa('[\\(\\)]',text)\n",
    "    \n",
    "    #POS & grammem\n",
    "    def cleanse(s):\n",
    "        rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "        return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "    \n",
    "    def parse_text(text, morph=morph):\n",
    "        tokens = word_tokenize(cleanse(text))\n",
    "        return [morph.parse(t) for t in tokens]\n",
    "    \n",
    "    parsed_text = parse_text(text)\n",
    "    pos_list = [str(p[0].tag.POS) for p in parsed_text]\n",
    "    n_nouns = len([t for t in pos_list if t=='NOUN'])\n",
    "    n_verbs = len([t for t in pos_list if t=='VERB'])\n",
    "    anim_list = [str(p[0].tag.animacy) for p in parsed_text]\n",
    "    pers_list = [str(p[0].tag.person) for p in parsed_text]\n",
    "    tns_list = [str(p[0].tag.tense) for p in parsed_text]\n",
    "    asp_list = [str(p[0].tag.aspect) for p in parsed_text]\n",
    "        \n",
    "    features = {\n",
    "        #surface features\n",
    "        'len_char': len_char, \n",
    "        'len_word': len_word,\n",
    "        'm_len_word': len_char / len_word,\n",
    "        #punctuation\n",
    "        'p_pun': len(pun) / len_char,\n",
    "        'p_dot': len([i for i in pun if i=='.'])/len(pun),\n",
    "        'p_qm': len([i for i in pun if i=='?'])/len(pun),\n",
    "        'p_excl': len([i for i in pun if i=='!'])/len(pun),\n",
    "        'p_comma': len([i for i in pun if i==','])/len(pun),\n",
    "        'p_brkt': len(braсket_list)/len_char,\n",
    "        'p_brkt_up': len([i for i in braсket_list if i==')'])/len(braсket_list),\n",
    "        #POS form\n",
    "        'pos_form': ' '.join(pos_list),\n",
    "        'pos_richness': len(set(pos_list)),\n",
    "        #grammem features\n",
    "        'p_anim': len([t for t in anim_list if t=='anim'])/n_nouns,\n",
    "        'p_1per': len([t for t in pers_list if t=='1per'])/n_verbs,\n",
    "        'p_3per': len([t for t in pers_list if t=='3per'])/n_verbs,\n",
    "        'p_past': len([t for t in tns_list if t=='past'])/n_verbs,\n",
    "        #'p_fut': len([t for t in tns_list if t=='futr'])/n_verbs,\n",
    "        'p_pres': len([t for t in tns_list if t=='pres'])/n_verbs,\n",
    "        'p_perf': len([t for t in asp_list if t=='perf'])/n_verbs,\n",
    "    }\n",
    "    \n",
    "    for f in pos_types:\n",
    "        features['p_'+f] = len([t for t in pos_list if t==f])/len(pos_list)\n",
    "        \n",
    "    \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'len_char': 1740,\n",
       " 'len_word': 277,\n",
       " 'm_len_word': 6.281588447653429,\n",
       " 'p_1per': 0.36666666666666664,\n",
       " 'p_3per': 0.43333333333333335,\n",
       " 'p_ADJF': 0.10108303249097472,\n",
       " 'p_ADVB': 0.1263537906137184,\n",
       " 'p_CONJ': 0.10830324909747292,\n",
       " 'p_NOUN': 0.2490974729241877,\n",
       " 'p_PREP': 0.10469314079422383,\n",
       " 'p_VERB': 0.10830324909747292,\n",
       " 'p_anim': 0.2463768115942029,\n",
       " 'p_brkt': 0.0011494252873563218,\n",
       " 'p_brkt_up': 0.5,\n",
       " 'p_comma': 0.4915254237288136,\n",
       " 'p_dot': 0.3728813559322034,\n",
       " 'p_excl': 0.01694915254237288,\n",
       " 'p_past': 0.5333333333333333,\n",
       " 'p_perf': 0.43333333333333335,\n",
       " 'p_pres': 0.5333333333333333,\n",
       " 'p_pun': 0.0339080459770115,\n",
       " 'p_qm': 0.0,\n",
       " 'pos_form': 'ADVB NPRO VERB PREP NOUN ADJF VERB PREP NPRO NOUN INFN ADJF NOUN ADVB PRCL VERB ADVB PREP NOUN NOUN NOUN VERB NOUN None PREP ADJF NOUN VERB COMP ADVB ADVB PREP NOUN CONJ PREP ADJF NOUN CONJ PRCL VERB ADVB COMP NPRO VERB CONJ PRCL NPRO PRCL NOUN NOUN CONJ PRCL ADVB VERB CONJ NOUN PRTF NOUN NOUN ADJF NOUN VERB NOUN CONJ PREP ADJF NOUN NOUN PREP ADJF NOUN ADVB PREP NOUN VERB NOUN ADVB VERB CONJ PREP NOUN NOUN CONJ NPRO ADVB VERB PREP NPRO NOUN INFN NPRO NOUN PRCL ADVB CONJ NPRO PRCL ADVB INFN PREP NOUN None ADVB PREP NOUN ADVB CONJ ADJF ADJF ADVB ADVB ADVB NOUN VERB INFN CONJ CONJ VERB PRTS CONJ NPRO PRCL PRCL VERB NOUN PREP NOUN NOUN None VERB NUMR NOUN ADVB VERB NUMR NOUN ADVB PRCL ADVB PREP NOUN NOUN NOUN CONJ PREP NOUN NOUN ADJF CONJ VERB NOUN ADVB PREP NPRO VERB ADJF NOUN CONJ PRTS NPRO ADVB ADVB CONJ ADVB CONJ PRCL INFN PREP ADJF NOUN ADVB PRCL PRED VERB PREP NOUN PREP ADJF VERB NOUN NUMR NOUN PRCL PRCL VERB NPRO NOUN ADJF ADVB CONJ PRCL ADVB ADJF None NPRO ADVB PRCL PREP NOUN VERB PREP ADJF NOUN ADJF NOUN NOUN VERB PREP ADJF NOUN NOUN PREP ADJF NOUN CONJ ADVB CONJ VERB ADJF NOUN CONJ PREP NOUN PREP None NOUN VERB NOUN CONJ ADJF NOUN None PRCL CONJ PREP ADJF NOUN ADJF ADJF NOUN NOUN NOUN NOUN PREP NOUN NOUN ADJF NOUN ADVB ADJS VERB INFN ADVB CONJ ADJF VERB NOUN CONJ INFN ADVB VERB ADVB CONJ ADVB CONJ VERB INFN CONJ ADJS CONJ ADJF NOUN NOUN PREP NPRO ADVB ADJF',\n",
       " 'pos_richness': 16}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extract_features(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
