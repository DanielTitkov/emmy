{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import findall as fa\n",
    "import sqlite3\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier \n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy.stats.stats import pearsonr as corr\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Иногда я пишу о книгах, которые произвели на меня впечатление. Писать большие отзывы сейчас не хочется, поэтому в порядке перечисления.\n",
    "\"Атлант расправил плечи\" - за последнее время понравилась больше всего наряду с Довлатовым (но насчет последнего сомнений и не было). Почему-то раньше я думал, что это что-то вроде \"Финансиста\" Драйзера. Так же, видимо, думают и люди, рисующие мемы \"сын маминой подруги расправил плечи\". А на самом деле книга об альтернативной вселенной, где в США наступил социализм. Очень рекомендую.\n",
    "Что до \"Финансиста\" Драйзера, то т.д. он надолго отбил у меня желание читать этого автора. Не потому что мне не интересно читать про рынок - наоборот, про рынок интересно. Но всё остальное там скучно, особенно герои. Может быть, так и было задумано, но я это не люблю.\n",
    "Дилогия об Остапе Бендере - начинаются обе книги весело, кончаются обе книги уныло. Не столько с точки зрения событий, сколько с точки зрения того, как трансформируется язык. Поэтому от них остается неприятное ощущение, хотя написаны они ярко, весело и интересно. Впрочем, не пойти на такую сделку вряд ли можно было в условиях, в которых работали авторы.\n",
    "\"Три мушкетера\". Ну, не побоюсь если б я хотел бы этого я слова, такое. Занятно, но не более того - я сейчас даже с трудом вспомнил об этой книжке. Главный интерес книжка представляет с исторической точки зрения. В том числе и потому, что является убедительным доказательством, что во Франции в 17м веке был интернет и портативные телепорты - ну или по крайней мере бесстыдная сценарная магия.\n",
    "Отто Кариус, \"Тигры в грязи\". необходимость обязана Язык Все всяк по видимому каждому каждый каждая этой можетбыть кажеться наверное наверно книги совершенно ужасен, может быть, потому что её писал солдат. Но прочитать очень стоит, потому что мало что может быть так ценно, как новая точка зрения на нечто хорошо знакомое!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' б ', ' бы ']"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa('\\sбы?\\s',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(s):\n",
    "    rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "    return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "\n",
    "def set_groups(x, dev=1, M=50, SD=10):\n",
    "    if x > M+dev*SD:\n",
    "        return 'high'\n",
    "    elif x < M-dev*SD:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'average'd\n",
    "    \n",
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обязана']"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imper = ['долж(?:ен|на|ны|но)', 'обязан(?:а|ы|о|)', \n",
    "         'надо\\W', 'нуж(?:но|ен|на|ны)', \n",
    "         'требуеть?ся', 'необходим(?:а|ы|о|)\\W']\n",
    "fa('|'.join(imper), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpos= ['PRED', 'None', 'PRTS', 'ADJF', 'INFN', \n",
    "         'PRTF', 'NOUN', 'ADVB', 'VERB', 'NPRO', \n",
    "         'NUMR', 'CONJ', 'ADJS', 'PRCL', 'PREP', 'COMP', 'INTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, \n",
    "                     morph=pymorphy2.MorphAnalyzer(), \n",
    "                     pos_types=['ADJF', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP', 'INTJ', 'None'],\n",
    "                     uncert = ['наверное?', 'может[\\s-]?быть', 'кажеть?ся', \n",
    "                               'видимо', 'возможно', 'по[\\s-]?видимому', \n",
    "                               'вероятно', 'должно[\\s-]?быть','пожалуй', 'как[\\s-]?видно'],\n",
    "                     cert = ['очевидно','конечно','точно','совершенно',\n",
    "                             'не\\s?сомненно','разумееть?ся', \n",
    "                             'по[\\s-]?любому','сто[\\s-]?пудово?'],\n",
    "                     quan = ['вс[её]x?','всегда','ни-?когда', 'постоянно', \n",
    "                             'ник(?:то|ого|ому|ем)', \n",
    "                             'кажд(?:ый|ая|ой|ому?|ое|ого|ую|ые|ою|ыми?|ых)',\n",
    "                             'всяк(?:ий|ая|ое|ого|ую|ому?|ой|ою|ими?|их|ие)',\n",
    "                             'люб(?:ой|ая|ое|ого|ому?|ую|ой|ыми?|ых|ые)'],\n",
    "                     imper = ['долж(?:ен|на|ны|но)', 'обязан(?:а|ы|о|)', \n",
    "                              'надо\\W', 'нуж(?:но|ен|на|ны)', \n",
    "                              'требуеть?ся', 'необходим(?:а|ы|о|)\\W'],\n",
    "                    ):\n",
    "    \n",
    "    #length in chars and words\n",
    "    len_char = len(text)\n",
    "    len_word = len(text.split())\n",
    "    len_sent = len(fa('[^\\.\\!\\?]+[\\.\\!\\?]', text))\n",
    "    len_sent = len_sent if len_sent else 1\n",
    "    pun = fa('[\\.+,!\\?:-]',text)\n",
    "    n_pun = len(pun)\n",
    "    braсket_list = fa('[\\(\\)]',text)\n",
    "      \n",
    "    #POS & grammem\n",
    "    def cleanse(s):\n",
    "        rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "        return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "    \n",
    "    def parse_text(text, morph=morph):\n",
    "        tokens = cleanse(text).split()\n",
    "        return [morph.parse(t) for t in tokens]\n",
    "    \n",
    "    parsed_text = parse_text(text)\n",
    "    pos_list = [str(p[0].tag.POS) for p in parsed_text]\n",
    "    n_nouns = len([t for t in pos_list if t=='NOUN'])\n",
    "    n_verbs = len([t for t in pos_list if t=='VERB'])\n",
    "    anim_list = [str(p[0].tag.animacy) for p in parsed_text]\n",
    "    pers_list = [str(p[0].tag.person) for p in parsed_text]\n",
    "    tns_list = [str(p[0].tag.tense) for p in parsed_text]\n",
    "    asp_list = [str(p[0].tag.aspect) for p in parsed_text]\n",
    "      \n",
    "    r = lambda x: round(x, 5)\n",
    "    d = lambda x, y: x / y if y else 0.0\n",
    "    \n",
    "    features = {\n",
    "        #surface features\n",
    "        'len_char': len_char, \n",
    "        'len_word': len_word,\n",
    "        'len_sent': len_sent,\n",
    "        'm_len_word': r(len_char / len_word),\n",
    "        'm_len_sent': r(len_word / len_sent),\n",
    "        #punctuation\n",
    "        'p_pun': r(len(pun) / len_char),\n",
    "        'p_dot': r(d(len([i for i in pun if i=='.']), len(pun))),\n",
    "        'p_qm': r(d(len([i for i in pun if i=='?']), len(pun))),\n",
    "        'p_excl': r(d(len([i for i in pun if i=='!']), len(pun))),\n",
    "        'p_comma': r(d(len([i for i in pun if i==',']), len(pun))),\n",
    "        'p_brkt': r(len(braсket_list) / len_char),\n",
    "        'p_brkt_up': r(d(len([i for i in braсket_list if i==')']), len(braсket_list))),\n",
    "        #POS form\n",
    "        'pos_form': ' '.join(pos_list),\n",
    "        'pos_richness': len(set(pos_list)),\n",
    "        #grammem features\n",
    "        'p_anim': r(d(len([t for t in anim_list if t=='anim']), n_nouns)),\n",
    "        'p_1per': r(d(len([t for t in pers_list if t=='1per']), n_verbs)),\n",
    "        'p_3per': r(d(len([t for t in pers_list if t=='3per']), n_verbs)),\n",
    "        'p_past': r(d(len([t for t in tns_list if t=='past']), n_verbs)),\n",
    "        #'p_fut': r(d(len([t for t in tns_list if t=='futr']), n_verbs)),\n",
    "        'p_pres': r(d(len([t for t in tns_list if t=='pres']), n_verbs)),\n",
    "        'p_perf': r(d(len([t for t in asp_list if t=='perf']), n_verbs)),\n",
    "        'p_conj': r(d(len(fa('\\sбы?\\s',text)), n_verbs)),\n",
    "        #lexical features\n",
    "        'p_uncert': r(len(fa('|'.join(uncert), text.lower())) / len_word),\n",
    "        'p_cert': r(len(fa('|'.join(cert), text.lower())) / len_word),\n",
    "        'p_quan': r(len(fa('|'.join(quan), text.lower())) / len_word),\n",
    "        'p_imper': r(len(fa('|'.join(imper), text.lower())) / len_word),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for f in pos_types:\n",
    "        features['p_'+f] = r(len([t for t in pos_list if t==f])/len(pos_list))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'len_char': 1866,\n",
       " 'len_sent': 25,\n",
       " 'len_word': 297,\n",
       " 'm_len_sent': 11.88,\n",
       " 'm_len_word': 6.28283,\n",
       " 'p_1per': 0.41935,\n",
       " 'p_3per': 0.41935,\n",
       " 'p_ADJF': 0.11074,\n",
       " 'p_ADVB': 0.13087,\n",
       " 'p_CONJ': 0.10403,\n",
       " 'p_INTJ': 0.0,\n",
       " 'p_NOUN': 0.23826,\n",
       " 'p_None': 0.02013,\n",
       " 'p_PREP': 0.10067,\n",
       " 'p_VERB': 0.10403,\n",
       " 'p_anim': 0.23944,\n",
       " 'p_brkt': 0.00107,\n",
       " 'p_brkt_up': 0.5,\n",
       " 'p_cert': 0.00337,\n",
       " 'p_comma': 0.47541,\n",
       " 'p_conj': 0.06452,\n",
       " 'p_dot': 0.39344,\n",
       " 'p_excl': 0.01639,\n",
       " 'p_imper': 0.00337,\n",
       " 'p_past': 0.58065,\n",
       " 'p_perf': 0.48387,\n",
       " 'p_pres': 0.51613,\n",
       " 'p_pun': 0.03269,\n",
       " 'p_qm': 0.0,\n",
       " 'p_quan': 0.02357,\n",
       " 'p_uncert': 0.0303,\n",
       " 'pos_form': 'ADVB NPRO VERB PREP NOUN ADJF VERB PREP NPRO NOUN INFN ADJF NOUN ADVB PRCL VERB ADVB PREP NOUN NOUN NOUN VERB NOUN None PREP ADJF NOUN VERB COMP ADVB ADVB PREP NOUN CONJ PREP ADJF NOUN CONJ PRCL VERB ADVB COMP NPRO VERB CONJ PRCL NPRO PRCL NOUN NOUN CONJ PRCL ADVB VERB CONJ NOUN PRTF NOUN NOUN ADJF NOUN VERB NOUN CONJ PREP ADJF NOUN NOUN PREP ADJF NOUN ADVB PREP NOUN VERB NOUN ADVB VERB CONJ PREP NOUN NOUN CONJ NOUN ADVB NPRO ADVB VERB PREP NPRO NOUN INFN NPRO NOUN PRCL ADVB CONJ NPRO PRCL ADVB INFN PREP NOUN None ADVB PREP NOUN ADVB CONJ ADJF ADJF ADVB ADVB ADVB NOUN VERB INFN CONJ CONJ VERB PRTS CONJ NPRO PRCL PRCL VERB NOUN PREP NOUN NOUN None VERB NUMR NOUN ADVB VERB NUMR NOUN ADVB PRCL ADVB PREP NOUN NOUN NOUN CONJ PREP NOUN NOUN ADJF CONJ VERB NOUN ADVB PREP NPRO VERB ADJF NOUN CONJ PRTS NPRO ADVB ADVB CONJ ADVB CONJ PRCL INFN PREP ADJF NOUN ADVB PRCL PRED VERB PREP NOUN PREP ADJF VERB NOUN NUMR NOUN PRCL PRCL VERB CONJ PRCL NPRO VERB PRCL NPRO NPRO NOUN ADJF ADVB CONJ PRCL ADVB ADJF None NPRO ADVB PRCL PREP NOUN VERB PREP ADJF NOUN ADJF NOUN NOUN VERB PREP ADJF NOUN NOUN PREP ADJF NOUN CONJ ADVB CONJ VERB ADJF NOUN CONJ PREP NOUN PREP None NOUN VERB NOUN CONJ ADJF NOUN None PRCL CONJ PREP ADJF NOUN ADJF ADJF NOUN NOUN NOUN NOUN PREP NOUN NOUN PRTS NOUN ADJF ADVB PREP ADJF ADJF ADJF ADJF ADJF INFN INFN ADVB ADVB NOUN ADVB ADJS VERB INFN ADVB CONJ ADJF VERB NOUN CONJ INFN ADVB VERB ADVB CONJ ADVB CONJ VERB INFN CONJ ADJS CONJ ADJF NOUN NOUN PREP NPRO ADVB ADJF',\n",
       " 'pos_richness': 16}"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extract_features(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_features(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1438, 2)"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "trsh_up, trsh_lo = 5000, 1000\n",
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate features: 100%|██████████████████████████████████████████████████████████| 1438/1438 [00:24<00:00, 58.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1438, 35)"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pymorphy2.MorphAnalyzer()\n",
    "tqdm.pandas(desc=\"Calculate features\")\n",
    "df_feat = pd.DataFrame.from_records(list(texts.text.progress_apply(extract_features, morph=m)))\n",
    "df_feat.index = texts.index\n",
    "texts_feat = pd.concat([texts, df_feat], axis=1, join='inner')\n",
    "feat_names = list(extract_features('ы').keys())\n",
    "feat_names.remove('pos_form')\n",
    "texts_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trait high average low\n",
      "HEX1_eX [53, 51, 48]\n",
      "HEX2_A [58, 51, 43]\n",
      "HEX3_C [53, 52, 47]\n",
      "HEX4_E [57, 48, 47]\n",
      "HEX5_O [56, 50, 46]\n",
      "HEX6_H [54, 50, 48]\n",
      "TWf1_eX [57, 50, 45]\n",
      "TWf2_A [55, 51, 46]\n",
      "TWf3_C [62, 47, 43]\n",
      "TWf4_E [53, 50, 49]\n",
      "TWf5_O [57, 49, 46]\n",
      "TWf6_H [53, 51, 48]\n",
      "TWc1_eX [59, 49, 44]\n",
      "TWc2_A [53, 52, 47]\n",
      "TWc3_C [54, 52, 46]\n",
      "TWc4_N [58, 49, 45]\n",
      "TWc5_O [74, 40, 38]\n",
      "TWc6_H [68, 44, 40]\n",
      "M1_eX [61, 46, 45]\n",
      "M2_A [59, 50, 43]\n",
      "M3_C [64, 47, 41]\n",
      "M4_E [69, 43, 40]\n",
      "M5_O [65, 45, 42]\n",
      "M6_H [66, 45, 41]\n"
     ]
    }
   ],
   "source": [
    "#load psychological data and transform traits\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H', \n",
    "        'TWf1_eX', 'TWf2_A', 'TWf3_C', 'TWf4_E', 'TWf5_O', 'TWf6_H', \n",
    "        'TWc1_eX', 'TWc2_A', 'TWc3_C', 'TWc4_N', 'TWc5_O', 'TWc6_H']\n",
    "\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "\n",
    "names_HEX = ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "names_TWf = ['TWf1_eX', 'TWf2_A', 'TWf3_C', 'TWf4_E', 'TWf5_O', 'TWf6_H']\n",
    "names_TWc = ['TWc1_eX', 'TWc2_A', 'TWc3_C', 'TWc4_N', 'TWc5_O', 'TWc6_H']\n",
    "names_M = ['M'+i[3:] for i in trait_names_HEX]\n",
    "\n",
    "for i, t in enumerate(names_M):\n",
    "    traits[t] = (traits[names_HEX[i]] + traits[names_TWf[i]] + traits[names_TWc[i]])/3\n",
    "\n",
    "trait_names = names_HEX + names_TWf + names_TWc + names_M\n",
    "\n",
    "print('trait high average low')\n",
    "for trait in trait_names:\n",
    "    scale = trait + '_nom'\n",
    "    traits[scale] = traits[trait].apply(set_groups, dev=0.5)\n",
    "    print(trait, [traits[scale].value_counts()[i] for i in range(3)])\n",
    "    \n",
    "trait_names = names_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1474, 85)"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data = pd.merge(texts_feat, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data.text = data.text.apply(cleanse)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: 1326\n",
      "Test sample: 148\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.1)\n",
    "print('Train sample: {}\\nTest sample: {}'.format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS\n",
      "\n",
      "Included tokens (1487)\n",
      "['всего' 'ноги' 'само' 'you' 'ней' 'это очень' 'тему' 'душу' 'тот кто'\n",
      " 'своей' 'смерти' 'делала' 'спасибо' 'та' 'стоит' 'пришел' 'числе' '10'\n",
      " 'при этом' 'то есть']\n",
      "\n",
      "Excluded tokens (399160)\n",
      "['по старому преображение' 'собственно вот сей' 'поэтка' 'рук я еду'\n",
      " 'кошмара если честно' 'проект за' 'собственного сочинения читающих'\n",
      " 'курс обучения' 'подобная встреча' 'dad' 'возгласы исполненные гнева'\n",
      " 'париться реально' 'личной эффективностью' 'обтекало кровью' 'гавани'\n",
      " 'танюшу за прекрасную' 'помнишь в рождество…' 'практически не получается'\n",
      " 'совершенно необходимо' 'это нужно никому']\n",
      "\n",
      "POS\n",
      "\n",
      "Included tokens (3504)\n",
      "['noun conj none' 'prep npro none adjf' 'noun none none noun'\n",
      " 'adjf npro noun conj' 'intj npro' 'none npro adjf' 'adjf advb verb prep'\n",
      " 'noun verb prcl adjf' 'noun none conj prcl' 'noun none verb prep'\n",
      " 'prcl advb adjf noun' 'noun advb prep npro' 'noun npro verb noun'\n",
      " 'prep noun conj prcl' 'adjf conj prcl noun' 'npro verb verb prep'\n",
      " 'verb advb noun verb' 'verb prcl infn conj' 'advb verb noun verb'\n",
      " 'npro adjf noun conj']\n",
      "\n",
      "Excluded tokens (399160)\n",
      "['pred verb adjf noun' 'noun noun comp verb' 'comp verb conj adjf'\n",
      " 'infn advb infn' 'noun noun prep adjf' 'none none noun prts'\n",
      " 'intj advb adjf noun' 'none infn infn adjf' 'comp advb conj prep'\n",
      " 'adjf conj noun prts' 'noun conj adjf' 'none prep adjf pred'\n",
      " 'noun conj pred comp' 'noun advb adjs prep' 'advb noun npro prep'\n",
      " 'prcl adjs noun noun' 'conj prtf npro comp' 'prcl verb verb prep'\n",
      " 'adjf infn conj verb' 'prtf conj advb advb']\n",
      "(1326, 5023) (148, 5023)\n"
     ]
    }
   ],
   "source": [
    "#prepare X\n",
    "X_train = train.loc[:,feat_names]\n",
    "X_test = test.loc[:,feat_names]\n",
    "\n",
    "#words tf:idf\n",
    "vect_words = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                     analyzer='word', \n",
    "                     tokenizer=word_tokenize, \n",
    "                     min_df = 30, \n",
    "                     max_df = 0.3, \n",
    "                     max_features = 10000)\n",
    "\n",
    "train_w_vec = vect_words.fit_transform(train.loc[:,'text'])\n",
    "test_w_vec = vect_words.transform(test.loc[:,'text'])\n",
    "\n",
    "print('WORDS')\n",
    "print('\\nIncluded tokens ({})'.format(train_w_vec.shape[1]))\n",
    "print(np.array(vect_words.get_feature_names())[np.random.randint(0, len(vect_words.get_feature_names()), 20)])\n",
    "print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "print(np.array(list(vect_words.stop_words_))[np.random.randint(0, len(vect_words.stop_words_), 20)])\n",
    "\n",
    "#pos tf:idf\n",
    "vect_pos = TfidfVectorizer(ngram_range=(2, 4), \n",
    "                     analyzer='word',  \n",
    "                     min_df = 30, \n",
    "                     max_df = 0.3, \n",
    "                     max_features = 10000)\n",
    "train_p_vec = vect_pos.fit_transform(train.loc[:,'pos_form'])\n",
    "test_p_vec = vect_pos.transform(test.loc[:,'pos_form'])\n",
    "\n",
    "print('\\nPOS')\n",
    "print('\\nIncluded tokens ({})'.format(train_p_vec.shape[1]))\n",
    "print(np.array(vect_pos.get_feature_names())[np.random.randint(0, len(vect_pos.get_feature_names()), 20)])\n",
    "print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "print(np.array(list(vect_pos.stop_words_))[np.random.randint(0, len(vect_pos.stop_words_), 20)])\n",
    "\n",
    "X_train = np.hstack((train_w_vec.todense(), train_p_vec.todense(), train.loc[:,feat_names]))\n",
    "X_test = np.hstack((test_w_vec.todense(), test_p_vec.todense(), test.loc[:,feat_names]))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "M1_eX\n",
      "========================================\n",
      "\n",
      "m_len_sent | M1_eX : r = -0.11\n",
      "p_brkt_up | M1_eX : r = -0.17\n",
      "p_anim | M1_eX : r = 0.13\n",
      "p_quan | M1_eX : r = 0.12\n",
      "p_VERB | M1_eX : r = 0.15\n",
      "p_None | M1_eX : r = -0.13\n",
      "\n",
      "========================================\n",
      "M2_A\n",
      "========================================\n",
      "\n",
      "len_sent | M2_A : r = 0.12\n",
      "m_len_sent | M2_A : r = -0.21\n",
      "p_dot | M2_A : r = 0.1\n",
      "pos_richness | M2_A : r = 0.19\n",
      "p_past | M2_A : r = -0.1\n",
      "p_pres | M2_A : r = 0.12\n",
      "p_perf | M2_A : r = 0.14\n",
      "p_ADJF | M2_A : r = 0.15\n",
      "p_NOUN | M2_A : r = 0.2\n",
      "p_ADVB | M2_A : r = -0.11\n",
      "p_VERB | M2_A : r = 0.18\n",
      "p_CONJ | M2_A : r = 0.11\n",
      "p_None | M2_A : r = -0.23\n",
      "\n",
      "========================================\n",
      "M3_C\n",
      "========================================\n",
      "\n",
      "p_anim | M3_C : r = -0.16\n",
      "p_past | M3_C : r = -0.22\n",
      "p_pres | M3_C : r = 0.18\n",
      "p_ADJF | M3_C : r = 0.11\n",
      "p_ADVB | M3_C : r = -0.1\n",
      "p_CONJ | M3_C : r = 0.12\n",
      "\n",
      "========================================\n",
      "M4_E\n",
      "========================================\n",
      "\n",
      "p_brkt | M4_E : r = 0.13\n",
      "p_brkt_up | M4_E : r = 0.25\n",
      "p_anim | M4_E : r = -0.14\n",
      "p_1per | M4_E : r = 0.2\n",
      "p_3per | M4_E : r = -0.21\n",
      "p_NOUN | M4_E : r = -0.15\n",
      "p_ADVB | M4_E : r = 0.13\n",
      "\n",
      "========================================\n",
      "M5_O\n",
      "========================================\n",
      "\n",
      "m_len_sent | M5_O : r = -0.12\n",
      "p_3per | M5_O : r = 0.14\n",
      "p_quan | M5_O : r = 0.1\n",
      "p_ADJF | M5_O : r = 0.13\n",
      "p_ADVB | M5_O : r = 0.11\n",
      "p_None | M5_O : r = -0.14\n",
      "\n",
      "========================================\n",
      "M6_H\n",
      "========================================\n",
      "\n",
      "p_brkt_up | M6_H : r = -0.14\n",
      "p_3per | M6_H : r = 0.11\n",
      "p_ADVB | M6_H : r = -0.12\n",
      "p_VERB | M6_H : r = 0.12\n"
     ]
    }
   ],
   "source": [
    "#correlations\n",
    "for trait in trait_names:\n",
    "    print('\\n{}\\n{}\\n{}\\n'.format('='*40,trait,'='*40))\n",
    "    for feat in feat_names:\n",
    "        cor = corr(data.loc[:,trait],data.loc[:,feat])\n",
    "        if abs(cor[0]) > 0.1:\n",
    "            print('{} | {} : r = {:.2}'.format(feat, trait, cor[0], cor[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_nom(X_train, X_test, y_train, y_test, vectorizer, model):\n",
    "    print('{}\\nBUILDING MODEL FOR {}\\n{}\\n'.format(\"=\"*40,y_train.name,\"=\"*40))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print('Accuracy on training sample: {:.2%}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "#     print(classification_report(y_train, y_train_pred))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print('Accuracy on test sample: {:.2%}'.format(accuracy_score(y_test, y_test_pred)))\n",
    "#     print(classification_report(y_test, y_test_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cont(X_train, X_test, y_train, y_test, vectorizer, model):\n",
    "    print('{}\\nBUILDING MODEL FOR {}\\n{}\\n'.format(\"=\"*40,y_train.name,\"=\"*40))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print('MAPE on training sample: {:.2f}%'.format(mape(y_train, y_train_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_train, y_train_pred)))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print('\\nMAPE on test sample: {:.2f}%'.format(mape(y_test, y_test_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_test, y_test_pred)))\n",
    "#     print('\\nHigh pole')\n",
    "#     [print(a) for a in sorted(list(zip(model.coef_, feat_names)), reverse=True)[0:5]]\n",
    "#     print('\\nLow pole')\n",
    "#     [print(a) for a in sorted(list(zip(model.coef_, feat_names)))[0:5]]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "BUILDING MODEL FOR M1_eX_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 88.91%\n",
      "Accuracy on test sample: 62.84%\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M2_A_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 90.80%\n",
      "Accuracy on test sample: 67.57%\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M3_C_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 89.44%\n",
      "Accuracy on test sample: 72.97%\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M4_E_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 88.01%\n",
      "Accuracy on test sample: 68.24%\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M5_O_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 87.41%\n",
      "Accuracy on test sample: 62.16%\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M6_H_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 83.03%\n",
      "Accuracy on test sample: 67.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trait in trait_names:\n",
    "    trait = trait+'_nom'\n",
    "    lm = RandomForestClassifier(n_estimators=500, max_features='log2', \n",
    "                                min_samples_leaf=20, oob_score = True)  \n",
    "    lm = LogisticRegression()\n",
    "#     lm = MultinomialNB()\n",
    "#     lm = GradientBoostingClassifier() #seems best as yet\n",
    "    build_model_nom(X_train=X_train, X_test=X_test, \n",
    "                y_train = train.loc[:,trait], y_test = test.loc[:,trait],\n",
    "                vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "BUILDING MODEL FOR M1_eX\n",
      "========================================\n",
      "\n",
      "MAPE on training sample: 16.58%\n",
      "R2 on training sample: 0.352\n",
      "\n",
      "MAPE on test sample: 19.16%\n",
      "R2 on training sample: 0.136\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M2_A\n",
      "========================================\n",
      "\n",
      "MAPE on training sample: 16.26%\n",
      "R2 on training sample: 0.382\n",
      "\n",
      "MAPE on test sample: 20.99%\n",
      "R2 on training sample: 0.154\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M3_C\n",
      "========================================\n",
      "\n",
      "MAPE on training sample: 16.12%\n",
      "R2 on training sample: 0.350\n",
      "\n",
      "MAPE on test sample: 19.98%\n",
      "R2 on training sample: 0.155\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M4_E\n",
      "========================================\n",
      "\n",
      "MAPE on training sample: 14.14%\n",
      "R2 on training sample: 0.368\n",
      "\n",
      "MAPE on test sample: 17.73%\n",
      "R2 on training sample: 0.137\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M5_O\n",
      "========================================\n",
      "\n",
      "MAPE on training sample: 16.73%\n",
      "R2 on training sample: 0.331\n",
      "\n",
      "MAPE on test sample: 20.91%\n",
      "R2 on training sample: 0.101\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR M6_H\n",
      "========================================\n",
      "\n",
      "MAPE on training sample: 8.81%\n",
      "R2 on training sample: 0.289\n",
      "\n",
      "MAPE on test sample: 9.91%\n",
      "R2 on training sample: 0.048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trait in trait_names:\n",
    "    lm = RandomForestRegressor(n_estimators=500, max_features='log2', \n",
    "                                min_samples_leaf=10, oob_score = True)  \n",
    "#     lm = LinearRegression()\n",
    "#     lm = GradientBoostingRegressor()\n",
    "    build_model_cont(X_train=X_train, X_test=X_test, \n",
    "                y_train = train.loc[:,trait], y_test = test.loc[:,trait],\n",
    "                vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
