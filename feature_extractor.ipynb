{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import findall as fa\n",
    "import sqlite3\n",
    "import pymorphy2\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Иногда я пишу о книгах, которые произвели на меня впечатление. Писать большие отзывы сейчас не хочется, поэтому в порядке перечисления.\n",
    "\"Атлант расправил плечи\" - за последнее время понравилась больше всего наряду с Довлатовым (но насчет последнего сомнений и не было). Почему-то раньше я думал, что это что-то вроде \"Финансиста\" Драйзера. Так же, видимо, думают и люди, рисующие мемы \"сын маминой подруги расправил плечи\". А на самом деле книга об альтернативной вселенной, где в США наступил социализм. Очень рекомендую.\n",
    "Что до \"Финансиста\" Драйзера, то т.д. он надолго отбил у меня желание читать этого автора. Не потому что мне не интересно читать про рынок - наоборот, про рынок интересно. Но всё остальное там скучно, особенно герои. Может быть, так и было задумано, но я это не люблю.\n",
    "Дилогия об Остапе Бендере - начинаются обе книги весело, кончаются обе книги уныло. Не столько с точки зрения событий, сколько с точки зрения того, как трансформируется язык. Поэтому от них остается неприятное ощущение, хотя написаны они ярко, весело и интересно. Впрочем, не пойти на такую сделку вряд ли можно было в условиях, в которых работали авторы.\n",
    "\"Три мушкетера\". Ну, не побоюсь этого я слова, такое. Занятно, но не более того - я сейчас даже с трудом вспомнил об этой книжке. Главный интерес книжка представляет с исторической точки зрения. В том числе и потому, что является убедительным доказательством, что во Франции в 17м веке был интернет и портативные телепорты - ну или по крайней мере бесстыдная сценарная магия.\n",
    "Отто Кариус, \"Тигры в грязи\". необходимость обязана Язык Все всяк по видимому каждому каждый каждая этой можетбыть кажеться наверное наверно книги совершенно ужасен, может быть, потому что её писал солдат. Но прочитать очень стоит, потому что мало что может быть так ценно, как новая точка зрения на нечто хорошо знакомое!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sents = len(fa('[^\\.\\!\\?]+[\\.\\!\\?]', text))\n",
    "n_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(s):\n",
    "    rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "    return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обязана']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncert = ['наверное?', 'может[\\s-]?быть', 'кажеть?ся', \n",
    "          'видимо', 'возможно', 'по[\\s-]?видимому', \n",
    "          'вероятно', 'должно[\\s-]?быть','пожалуй', 'как[\\s-]видно']\n",
    "cert = ['очевидно','конечно','точно','совершенно',\n",
    "        'не\\s?сомненно','разумееть?ся']\n",
    "quan = ['вс[её]x?','всегда','ни-?когда', 'постоянно', \n",
    "        'ник(?:то|ого|ому|ем)',\n",
    "        'кажд(?:ый|ая|ой|ому?|ое|ого|ую|ые|ою|ыми?|ых)',\n",
    "        'всяк(?:ий|ая|ое|ого|ую|ому?|ой|ою|ими?|их|ие)',\n",
    "        'люб(?:ой|ая|ое|ого|ому?|ую|ой|ыми?|ых|ые)']\n",
    "imper = ['долж(?:ен|на|ны|но)', 'обязан(?:а|ы|о|)', \n",
    "         'надо\\W', 'нуж(?:но|ен|на|ны)', \n",
    "         'требуеть?ся', 'необходим(?:а|ы|о|)\\W']\n",
    "fa('|'.join(imper), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpos= ['PRED', 'None', 'PRTS', 'ADJF', 'INFN', \n",
    "         'PRTF', 'NOUN', 'ADVB', 'VERB', 'NPRO', \n",
    "         'NUMR', 'CONJ', 'ADJS', 'PRCL', 'PREP', 'COMP', 'INTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, \n",
    "                     morph=pymorphy2.MorphAnalyzer(), \n",
    "                     pos_types=['ADJF', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP', 'INTJ'],\n",
    "                     uncert = ['наверное?', 'может[\\s-]?быть', 'кажеть?ся', \n",
    "                               'видимо', 'возможно', 'по[\\s-]?видимому', \n",
    "                               'вероятно', 'должно[\\s-]?быть','пожалуй', 'как[\\s-]?видно'],\n",
    "                     cert = ['очевидно','конечно','точно','совершенно',\n",
    "                             'не\\s?сомненно','разумееть?ся', \n",
    "                             'по[\\s-]?любому','сто[\\s-]?пудово?'],\n",
    "                     quan = ['вс[её]x?','всегда','ни-?когда', 'постоянно', \n",
    "                             'ник(?:то|ого|ому|ем)', \n",
    "                             'кажд(?:ый|ая|ой|ому?|ое|ого|ую|ые|ою|ыми?|ых)',\n",
    "                             'всяк(?:ий|ая|ое|ого|ую|ому?|ой|ою|ими?|их|ие)',\n",
    "                             'люб(?:ой|ая|ое|ого|ому?|ую|ой|ыми?|ых|ые)'],\n",
    "                     imper = ['долж(?:ен|на|ны|но)', 'обязан(?:а|ы|о|)', \n",
    "                              'надо\\W', 'нуж(?:но|ен|на|ны)', \n",
    "                              'требуеть?ся', 'необходим(?:а|ы|о|)\\W'],\n",
    "                    ):\n",
    "    \n",
    "    #length in chars and words\n",
    "    len_char = len(text)\n",
    "    len_word = len(text.split())\n",
    "    len_sent = len(fa('[^\\.\\!\\?]+[\\.\\!\\?]', text))\n",
    "    pun = fa('[\\.+,!\\?:-]',text)\n",
    "    n_pun = len(pun)\n",
    "    braсket_list = fa('[\\(\\)]',text)\n",
    "      \n",
    "    #POS & grammem\n",
    "    def cleanse(s):\n",
    "        rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "        return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "    \n",
    "    def parse_text(text, morph=morph):\n",
    "        tokens = cleanse(text).split()\n",
    "        return [morph.parse(t) for t in tokens]\n",
    "    \n",
    "    parsed_text = parse_text(text)\n",
    "    pos_list = [str(p[0].tag.POS) for p in parsed_text]\n",
    "    n_nouns = len([t for t in pos_list if t=='NOUN'])\n",
    "    n_verbs = len([t for t in pos_list if t=='VERB'])\n",
    "    anim_list = [str(p[0].tag.animacy) for p in parsed_text]\n",
    "    pers_list = [str(p[0].tag.person) for p in parsed_text]\n",
    "    tns_list = [str(p[0].tag.tense) for p in parsed_text]\n",
    "    asp_list = [str(p[0].tag.aspect) for p in parsed_text]\n",
    "      \n",
    "    r = lambda x: round(x, 5)\n",
    "    \n",
    "    features = {\n",
    "        #surface features\n",
    "        'len_char': len_char, \n",
    "        'len_word': len_word,\n",
    "        'len_sent': len_sent,\n",
    "        'm_len_word': r(len_char / len_word),\n",
    "        'm_len_sent': r(len_word / len_sent),\n",
    "        #punctuation\n",
    "        'p_pun': r(len(pun) / len_char),\n",
    "        'p_dot': r(len([i for i in pun if i=='.']) / len(pun)),\n",
    "        'p_qm': r(len([i for i in pun if i=='?']) / len(pun)),\n",
    "        'p_excl': r(len([i for i in pun if i=='!']) / len(pun)),\n",
    "        'p_comma': r(len([i for i in pun if i==',']) / len(pun)),\n",
    "        'p_brkt': r(len(braсket_list) / len_char),\n",
    "        'p_brkt_up': r(len([i for i in braсket_list if i==')']) / len(braсket_list)),\n",
    "        #POS form\n",
    "        #'pos_form': ' '.join(pos_list),\n",
    "        'pos_richness': len(set(pos_list)),\n",
    "        #grammem features\n",
    "        'p_anim': r(len([t for t in anim_list if t=='anim']) / n_nouns),\n",
    "        'p_1per': r(len([t for t in pers_list if t=='1per']) / n_verbs),\n",
    "        'p_3per': r(len([t for t in pers_list if t=='3per']) / n_verbs),\n",
    "        'p_past': r(len([t for t in tns_list if t=='past']) / n_verbs),\n",
    "        #'p_fut': r(len([t for t in tns_list if t=='futr']) / n_verbs),\n",
    "        'p_pres': r(len([t for t in tns_list if t=='pres']) / n_verbs),\n",
    "        'p_perf': r(len([t for t in asp_list if t=='perf']) / n_verbs),\n",
    "        #lexical features\n",
    "        'p_uncert': r(len(fa('|'.join(uncert), text.lower())) / len_word),\n",
    "        'p_cert': r(len(fa('|'.join(cert), text.lower())) / len_word),\n",
    "        'p_quan': r(len(fa('|'.join(quan), text.lower())) / len_word),\n",
    "        'p_imper': r(len(fa('|'.join(imper), text.lower())) / len_word),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for f in pos_types:\n",
    "        features['p_'+f] = r(len([t for t in pos_list if t==f])/len(pos_list))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'len_char': 1848,\n",
       " 'len_sent': 25,\n",
       " 'len_word': 292,\n",
       " 'm_len_sent': 11.68,\n",
       " 'm_len_word': 6.32877,\n",
       " 'p_1per': 0.4,\n",
       " 'p_3per': 0.43333,\n",
       " 'p_ADJF': 0.11263,\n",
       " 'p_ADVB': 0.13311,\n",
       " 'p_CONJ': 0.10239,\n",
       " 'p_INTJ': 0.0,\n",
       " 'p_NOUN': 0.24232,\n",
       " 'p_PREP': 0.10239,\n",
       " 'p_VERB': 0.10239,\n",
       " 'p_anim': 0.23944,\n",
       " 'p_brkt': 0.00108,\n",
       " 'p_brkt_up': 0.5,\n",
       " 'p_cert': 0.00342,\n",
       " 'p_comma': 0.47541,\n",
       " 'p_dot': 0.39344,\n",
       " 'p_excl': 0.01639,\n",
       " 'p_imper': 0.00342,\n",
       " 'p_past': 0.56667,\n",
       " 'p_perf': 0.5,\n",
       " 'p_pres': 0.53333,\n",
       " 'p_pun': 0.03301,\n",
       " 'p_qm': 0.0,\n",
       " 'p_quan': 0.02397,\n",
       " 'p_uncert': 0.03082,\n",
       " 'pos_richness': 16}"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extract_features(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_features(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
