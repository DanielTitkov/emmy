{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import findall as fa\n",
    "import sqlite3\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier \n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy.stats.stats import pearsonr as corr\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Иногда я пишу о книгах, которые произвели на меня впечатление. Писать большие отзывы сейчас не хочется, поэтому в порядке перечисления.\n",
    "\"Атлант расправил плечи\" - за последнее время понравилась больше всего наряду с Довлатовым (но насчет последнего сомнений и не было). Почему-то раньше я думал, что это что-то вроде \"Финансиста\" Драйзера. Так же, видимо, думают и люди, рисующие мемы \"сын маминой подруги расправил плечи\". А на самом деле книга об альтернативной вселенной, где в США наступил социализм. Очень рекомендую.\n",
    "Что до \"Финансиста\" Драйзера, то т.д. он надолго отбил у меня желание читать этого автора. Не потому что мне не интересно читать про рынок - наоборот, про рынок интересно. Но всё остальное там скучно, особенно герои. Может быть, так и было задумано, но я это не люблю.\n",
    "Дилогия об Остапе Бендере - начинаются обе книги весело, кончаются обе книги уныло. Не столько с точки зрения событий, сколько с точки зрения того, как трансформируется язык. Поэтому от них остается неприятное ощущение, хотя написаны они ярко, весело и интересно. Впрочем, не пойти на такую сделку вряд ли можно было в условиях, в которых работали авторы.\n",
    "\"Три мушкетера\". Ну, не побоюсь если б я хотел бы этого я слова, такое. Занятно, но не более того - я сейчас даже с трудом вспомнил об этой книжке. Главный интерес книжка представляет с исторической точки зрения. В том числе и потому, что является убедительным доказательством, что во Франции в 17м веке был интернет и портативные телепорты - ну или по крайней мере бесстыдная сценарная магия.\n",
    "Отто Кариус, \"Тигры в грязи\". необходимость обязана Язык Все всяк по видимому каждому каждый каждая этой можетбыть кажеться наверное наверно книги совершенно ужасен, может быть, потому что её писал солдат. Но прочитать очень стоит, потому что мало что может быть так ценно, как новая точка зрения на нечто хорошо знакомое!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' б ', ' бы ']"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa('\\sбы?\\s',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(s):\n",
    "    rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "    return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "\n",
    "def set_groups(x, dev=1, M=50, SD=10):\n",
    "    if x > M+dev*SD:\n",
    "        return 'high'\n",
    "    elif x < M-dev*SD:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'average'\n",
    "    \n",
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обязана']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncert = ['наверное?', 'может[\\s-]?быть', 'кажеть?ся', \n",
    "          'видимо', 'возможно', 'по[\\s-]?видимому', \n",
    "          'вероятно', 'должно[\\s-]?быть','пожалуй', 'как[\\s-]видно']\n",
    "cert = ['очевидно','конечно','точно','совершенно',\n",
    "        'не\\s?сомненно','разумееть?ся']\n",
    "quan = ['вс[её]x?','всегда','ни-?когда', 'постоянно', \n",
    "        'ник(?:то|ого|ому|ем)',\n",
    "        'кажд(?:ый|ая|ой|ому?|ое|ого|ую|ые|ою|ыми?|ых)',\n",
    "        'всяк(?:ий|ая|ое|ого|ую|ому?|ой|ою|ими?|их|ие)',\n",
    "        'люб(?:ой|ая|ое|ого|ому?|ую|ой|ыми?|ых|ые)']\n",
    "imper = ['долж(?:ен|на|ны|но)', 'обязан(?:а|ы|о|)', \n",
    "         'надо\\W', 'нуж(?:но|ен|на|ны)', \n",
    "         'требуеть?ся', 'необходим(?:а|ы|о|)\\W']\n",
    "fa('|'.join(imper), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpos= ['PRED', 'None', 'PRTS', 'ADJF', 'INFN', \n",
    "         'PRTF', 'NOUN', 'ADVB', 'VERB', 'NPRO', \n",
    "         'NUMR', 'CONJ', 'ADJS', 'PRCL', 'PREP', 'COMP', 'INTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, \n",
    "                     morph=pymorphy2.MorphAnalyzer(), \n",
    "                     pos_types=['ADJF', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP', 'INTJ', 'None'],\n",
    "                     uncert = ['наверное?', 'может[\\s-]?быть', 'кажеть?ся', \n",
    "                               'видимо', 'возможно', 'по[\\s-]?видимому', \n",
    "                               'вероятно', 'должно[\\s-]?быть','пожалуй', 'как[\\s-]?видно'],\n",
    "                     cert = ['очевидно','конечно','точно','совершенно',\n",
    "                             'не\\s?сомненно','разумееть?ся', \n",
    "                             'по[\\s-]?любому','сто[\\s-]?пудово?'],\n",
    "                     quan = ['вс[её]x?','всегда','ни-?когда', 'постоянно', \n",
    "                             'ник(?:то|ого|ому|ем)', \n",
    "                             'кажд(?:ый|ая|ой|ому?|ое|ого|ую|ые|ою|ыми?|ых)',\n",
    "                             'всяк(?:ий|ая|ое|ого|ую|ому?|ой|ою|ими?|их|ие)',\n",
    "                             'люб(?:ой|ая|ое|ого|ому?|ую|ой|ыми?|ых|ые)'],\n",
    "                     imper = ['долж(?:ен|на|ны|но)', 'обязан(?:а|ы|о|)', \n",
    "                              'надо\\W', 'нуж(?:но|ен|на|ны)', \n",
    "                              'требуеть?ся', 'необходим(?:а|ы|о|)\\W'],\n",
    "                    ):\n",
    "    \n",
    "    #length in chars and words\n",
    "    len_char = len(text)\n",
    "    len_word = len(text.split())\n",
    "    len_sent = len(fa('[^\\.\\!\\?]+[\\.\\!\\?]', text))\n",
    "    len_sent = len_sent if len_sent else 1\n",
    "    pun = fa('[\\.+,!\\?:-]',text)\n",
    "    n_pun = len(pun)\n",
    "    braсket_list = fa('[\\(\\)]',text)\n",
    "      \n",
    "    #POS & grammem\n",
    "    def cleanse(s):\n",
    "        rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "        return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))\n",
    "    \n",
    "    def parse_text(text, morph=morph):\n",
    "        tokens = cleanse(text).split()\n",
    "        return [morph.parse(t) for t in tokens]\n",
    "    \n",
    "    parsed_text = parse_text(text)\n",
    "    pos_list = [str(p[0].tag.POS) for p in parsed_text]\n",
    "    n_nouns = len([t for t in pos_list if t=='NOUN'])\n",
    "    n_verbs = len([t for t in pos_list if t=='VERB'])\n",
    "    anim_list = [str(p[0].tag.animacy) for p in parsed_text]\n",
    "    pers_list = [str(p[0].tag.person) for p in parsed_text]\n",
    "    tns_list = [str(p[0].tag.tense) for p in parsed_text]\n",
    "    asp_list = [str(p[0].tag.aspect) for p in parsed_text]\n",
    "      \n",
    "    r = lambda x: round(x, 5)\n",
    "    d = lambda x, y: x / y if y else 0.0\n",
    "    \n",
    "    features = {\n",
    "        #surface features\n",
    "        'len_char': len_char, \n",
    "        'len_word': len_word,\n",
    "        'len_sent': len_sent,\n",
    "        'm_len_word': r(len_char / len_word),\n",
    "        'm_len_sent': r(len_word / len_sent),\n",
    "        #punctuation\n",
    "        'p_pun': r(len(pun) / len_char),\n",
    "        'p_dot': r(d(len([i for i in pun if i=='.']), len(pun))),\n",
    "        'p_qm': r(d(len([i for i in pun if i=='?']), len(pun))),\n",
    "        'p_excl': r(d(len([i for i in pun if i=='!']), len(pun))),\n",
    "        'p_comma': r(d(len([i for i in pun if i==',']), len(pun))),\n",
    "        'p_brkt': r(len(braсket_list) / len_char),\n",
    "        'p_brkt_up': r(d(len([i for i in braсket_list if i==')']), len(braсket_list))),\n",
    "        #POS form\n",
    "        'pos_form': ' '.join(pos_list),\n",
    "        'pos_richness': len(set(pos_list)),\n",
    "        #grammem features\n",
    "        'p_anim': r(d(len([t for t in anim_list if t=='anim']), n_nouns)),\n",
    "        'p_1per': r(d(len([t for t in pers_list if t=='1per']), n_verbs)),\n",
    "        'p_3per': r(d(len([t for t in pers_list if t=='3per']), n_verbs)),\n",
    "        'p_past': r(d(len([t for t in tns_list if t=='past']), n_verbs)),\n",
    "        #'p_fut': r(d(len([t for t in tns_list if t=='futr']), n_verbs)),\n",
    "        'p_pres': r(d(len([t for t in tns_list if t=='pres']), n_verbs)),\n",
    "        'p_perf': r(d(len([t for t in asp_list if t=='perf']), n_verbs)),\n",
    "        'p_conj': r(d(len(fa('\\sбы?\\s',text)), n_verbs)),\n",
    "        #lexical features\n",
    "        'p_uncert': r(len(fa('|'.join(uncert), text.lower())) / len_word),\n",
    "        'p_cert': r(len(fa('|'.join(cert), text.lower())) / len_word),\n",
    "        'p_quan': r(len(fa('|'.join(quan), text.lower())) / len_word),\n",
    "        'p_imper': r(len(fa('|'.join(imper), text.lower())) / len_word),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for f in pos_types:\n",
    "        features['p_'+f] = r(len([t for t in pos_list if t==f])/len(pos_list))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'len_char': 1866,\n",
       " 'len_sent': 25,\n",
       " 'len_word': 297,\n",
       " 'm_len_sent': 11.88,\n",
       " 'm_len_word': 6.28283,\n",
       " 'p_1per': 0.41935,\n",
       " 'p_3per': 0.41935,\n",
       " 'p_ADJF': 0.11074,\n",
       " 'p_ADVB': 0.13087,\n",
       " 'p_CONJ': 0.10403,\n",
       " 'p_INTJ': 0.0,\n",
       " 'p_NOUN': 0.23826,\n",
       " 'p_None': 0.02013,\n",
       " 'p_PREP': 0.10067,\n",
       " 'p_VERB': 0.10403,\n",
       " 'p_anim': 0.23944,\n",
       " 'p_brkt': 0.00107,\n",
       " 'p_brkt_up': 0.5,\n",
       " 'p_cert': 0.00337,\n",
       " 'p_comma': 0.47541,\n",
       " 'p_conj': 0.06452,\n",
       " 'p_dot': 0.39344,\n",
       " 'p_excl': 0.01639,\n",
       " 'p_imper': 0.00337,\n",
       " 'p_past': 0.58065,\n",
       " 'p_perf': 0.48387,\n",
       " 'p_pres': 0.51613,\n",
       " 'p_pun': 0.03269,\n",
       " 'p_qm': 0.0,\n",
       " 'p_quan': 0.02357,\n",
       " 'p_uncert': 0.0303,\n",
       " 'pos_form': 'ADVB NPRO VERB PREP NOUN ADJF VERB PREP NPRO NOUN INFN ADJF NOUN ADVB PRCL VERB ADVB PREP NOUN NOUN NOUN VERB NOUN None PREP ADJF NOUN VERB COMP ADVB ADVB PREP NOUN CONJ PREP ADJF NOUN CONJ PRCL VERB ADVB COMP NPRO VERB CONJ PRCL NPRO PRCL NOUN NOUN CONJ PRCL ADVB VERB CONJ NOUN PRTF NOUN NOUN ADJF NOUN VERB NOUN CONJ PREP ADJF NOUN NOUN PREP ADJF NOUN ADVB PREP NOUN VERB NOUN ADVB VERB CONJ PREP NOUN NOUN CONJ NOUN ADVB NPRO ADVB VERB PREP NPRO NOUN INFN NPRO NOUN PRCL ADVB CONJ NPRO PRCL ADVB INFN PREP NOUN None ADVB PREP NOUN ADVB CONJ ADJF ADJF ADVB ADVB ADVB NOUN VERB INFN CONJ CONJ VERB PRTS CONJ NPRO PRCL PRCL VERB NOUN PREP NOUN NOUN None VERB NUMR NOUN ADVB VERB NUMR NOUN ADVB PRCL ADVB PREP NOUN NOUN NOUN CONJ PREP NOUN NOUN ADJF CONJ VERB NOUN ADVB PREP NPRO VERB ADJF NOUN CONJ PRTS NPRO ADVB ADVB CONJ ADVB CONJ PRCL INFN PREP ADJF NOUN ADVB PRCL PRED VERB PREP NOUN PREP ADJF VERB NOUN NUMR NOUN PRCL PRCL VERB CONJ PRCL NPRO VERB PRCL NPRO NPRO NOUN ADJF ADVB CONJ PRCL ADVB ADJF None NPRO ADVB PRCL PREP NOUN VERB PREP ADJF NOUN ADJF NOUN NOUN VERB PREP ADJF NOUN NOUN PREP ADJF NOUN CONJ ADVB CONJ VERB ADJF NOUN CONJ PREP NOUN PREP None NOUN VERB NOUN CONJ ADJF NOUN None PRCL CONJ PREP ADJF NOUN ADJF ADJF NOUN NOUN NOUN NOUN PREP NOUN NOUN PRTS NOUN ADJF ADVB PREP ADJF ADJF ADJF ADJF ADJF INFN INFN ADVB ADVB NOUN ADVB ADJS VERB INFN ADVB CONJ ADJF VERB NOUN CONJ INFN ADVB VERB ADVB CONJ ADVB CONJ VERB INFN CONJ ADJS CONJ ADJF NOUN NOUN PREP NPRO ADVB ADJF',\n",
       " 'pos_richness': 16}"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extract_features(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_features(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "trsh_up, trsh_lo = 700, 200\n",
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate features: 100%|█████████████████████████████████████████████████████████| 4298/4298 [00:19<00:00, 223.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4298, 35)"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pymorphy2.MorphAnalyzer()\n",
    "tqdm.pandas(desc=\"Calculate features\")\n",
    "df_feat = pd.DataFrame.from_records(list(texts.text.progress_apply(extract_features, morph=m)))\n",
    "df_feat.index = texts.index\n",
    "texts_feat = pd.concat([texts, df_feat], axis=1, join='inner')\n",
    "feat_names = list(extract_features('ы').keys())\n",
    "feat_names.remove('pos_form')\n",
    "texts_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trait high average low\n",
      "HEX1_eX [53, 51, 48]\n",
      "HEX2_A [58, 51, 43]\n",
      "HEX3_C [53, 52, 47]\n",
      "HEX4_E [57, 48, 47]\n",
      "HEX5_O [56, 50, 46]\n",
      "HEX6_H [54, 50, 48]\n"
     ]
    }
   ],
   "source": [
    "#load psychological data and transform traits\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "trait_names = ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "print('trait high average low')\n",
    "for trait in trait_names:\n",
    "    scale = trait + '_nom'\n",
    "    traits[scale] = traits[trait].apply(set_groups, dev=0.5)\n",
    "    print(trait, [traits[scale].value_counts()[i] for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4532, 49)"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data = pd.merge(texts_feat, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data.text = data.text.apply(cleanse)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: 4078\n",
      "Test sample: 454\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.1)\n",
    "print('Train sample: {}\\nTest sample: {}'.format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS\n",
      "\n",
      "Included tokens (850)\n",
      "['лет' 'таких' 'иногда' 'свою' 'так что' 'получить' 'vk' 'было' 'чем-то'\n",
      " 'я в' 'звезды' 'мной' 'ведь' '15' 'связи' 'нашей' 'совсем не' 'другие'\n",
      " 'что-то' 'ответ']\n",
      "\n",
      "Excluded tokens (399160)\n",
      "['судье и просит' 'фаворитом' 'закачался' 'едой и всем-всем'\n",
      " 'с ледяной водой' 'the only way' 'меня 31' 'crumpets there' 'у кого как'\n",
      " 'на консультацию к' 'твоих руках лежат' 'всему своё' 'поступки но в'\n",
      " 'стремятся не допустить' 'жизнь мой рагнарёк' 'священник и'\n",
      " 'стремление человека к' 'сдали' 'словно бросал' 'зарев и']\n",
      "POS\n",
      "\n",
      "Included tokens (2385)\n",
      "['npro infn npro' 'verb pred' 'none adjf' 'prep noun advb conj'\n",
      " 'infn prep adjf' 'infn pred' 'verb none conj' 'verb npro verb'\n",
      " 'noun npro adjs' 'adjf noun noun conj' 'conj adjf prep'\n",
      " 'noun infn noun conj' 'prep npro noun' 'noun noun advb advb'\n",
      " 'adjf conj verb noun' 'prcl advb adjf noun' 'none adjf adjf'\n",
      " 'adjf adjf adjf noun' 'verb conj advb' 'conj noun noun verb']\n",
      "\n",
      "Excluded tokens (399160)\n",
      "['conj conj infn prcl' 'verb pred prcl' 'conj noun comp advb'\n",
      " 'conj infn noun grnd' 'conj verb pred prcl' 'verb adjf adjf prep'\n",
      " 'adjf npro adjf prep' 'npro advb infn npro' 'noun conj prtf noun'\n",
      " 'prcl intj advb conj' 'none comp prcl conj' 'infn comp none none'\n",
      " 'infn verb advb verb' 'conj advb conj conj' 'verb adjs advb prcl'\n",
      " 'adjs npro prcl npro' 'prep none advb conj' 'npro prcl infn prep'\n",
      " 'prcl grnd adjf' 'prep noun grnd noun']\n"
     ]
    }
   ],
   "source": [
    "#prepare X\n",
    "X_train = train.loc[:,feat_names]\n",
    "X_test = test.loc[:,feat_names]\n",
    "\n",
    "#words tf:idf\n",
    "vect_words = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                     analyzer='word', \n",
    "                     tokenizer=word_tokenize, \n",
    "                     min_df = 30, \n",
    "                     max_df = 0.3, \n",
    "                     max_features = 10000)\n",
    "\n",
    "train_w_vec = vect_words.fit_transform(train.loc[:,'text'])\n",
    "test_w_vec = vect_words.transform(test.loc[:,'text'])\n",
    "\n",
    "print('WORDS')\n",
    "print('\\nIncluded tokens ({})'.format(train_w_vec.shape[1]))\n",
    "print(np.array(vect_words.get_feature_names())[np.random.randint(0, len(vect_words.get_feature_names()), 20)])\n",
    "print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "print(np.array(list(vect_words.stop_words_))[np.random.randint(0, len(vect_words.stop_words_), 20)])\n",
    "\n",
    "#pos tf:idf\n",
    "vect_pos = TfidfVectorizer(ngram_range=(2, 4), \n",
    "                     analyzer='word',  \n",
    "                     min_df = 30, \n",
    "                     max_df = 0.3, \n",
    "                     max_features = 10000)\n",
    "train_p_vec = vect_pos.fit_transform(train.loc[:,'pos_form'])\n",
    "test_p_vec = vect_pos.transform(test.loc[:,'pos_form'])\n",
    "\n",
    "print('POS')\n",
    "print('\\nIncluded tokens ({})'.format(train_p_vec.shape[1]))\n",
    "print(np.array(vect_pos.get_feature_names())[np.random.randint(0, len(vect_pos.get_feature_names()), 20)])\n",
    "print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "print(np.array(list(vect_pos.stop_words_))[np.random.randint(0, len(vect_pos.stop_words_), 20)])\n",
    "\n",
    "X_train = np.hstack((train_w_vec.todense(), train_p_vec.todense()))\n",
    "X_test = np.hstack((test_w_vec.todense(), test_p_vec.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "HEX1_eX\n",
      "========================================\n",
      "\n",
      "p_qm | HEX1_eX : r = -0.1\n",
      "p_comma | HEX1_eX : r = 0.18\n",
      "p_3per | HEX1_eX : r = 0.13\n",
      "p_ADJF | HEX1_eX : r = 0.12\n",
      "p_None | HEX1_eX : r = -0.13\n",
      "\n",
      "========================================\n",
      "HEX2_A\n",
      "========================================\n",
      "\n",
      "p_comma | HEX2_A : r = 0.11\n",
      "p_quan | HEX2_A : r = 0.1\n",
      "p_ADJF | HEX2_A : r = 0.1\n",
      "\n",
      "========================================\n",
      "HEX3_C\n",
      "========================================\n",
      "\n",
      "p_comma | HEX3_C : r = 0.11\n",
      "p_past | HEX3_C : r = -0.14\n",
      "p_pres | HEX3_C : r = 0.1\n",
      "p_quan | HEX3_C : r = 0.12\n",
      "\n",
      "========================================\n",
      "HEX4_E\n",
      "========================================\n",
      "\n",
      "p_1per | HEX4_E : r = 0.12\n",
      "\n",
      "========================================\n",
      "HEX5_O\n",
      "========================================\n",
      "\n",
      "\n",
      "========================================\n",
      "HEX6_H\n",
      "========================================\n",
      "\n",
      "p_quan | HEX6_H : r = 0.12\n"
     ]
    }
   ],
   "source": [
    "#correlations\n",
    "for trait in trait_names:\n",
    "    print('\\n{}\\n{}\\n{}\\n'.format('='*40,trait,'='*40))\n",
    "    for feat in feat_names:\n",
    "        cor = corr(data.loc[:,trait],data.loc[:,feat])\n",
    "        if abs(cor[0]) > 0.1:\n",
    "            print('{} | {} : r = {:.2}'.format(feat, trait, cor[0], cor[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_nom(X_train, X_test, y_train, y_test, vectorizer, model):\n",
    "    print('{}\\nBUILDING MODEL FOR {}\\n{}\\n'.format(\"=\"*40,y_train.name,\"=\"*40))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print('Accuracy on training sample: {:.2f}%'.format(accuracy_score(y_train, y_train_pred)))\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print('Accuracy on test sample: {:.2f}%'.format(accuracy_score(y_test, y_test_pred)))\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cont(X_train, X_test, y_train, y_test, vectorizer, model):\n",
    "    print('{}\\nBUILDING MODEL FOR {}\\n{}\\n'.format(\"=\"*40,y_train.name,\"=\"*40))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print('MAPE on training sample: {:.2f}%'.format(mape(y_train, y_train_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_train, y_train_pred)))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print('\\nMAPE on test sample: {:.2f}%'.format(mape(y_test, y_test_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    print('\\nHigh pole')\n",
    "#     [print(a) for a in sorted(list(zip(model.coef_, feat_names)), reverse=True)[0:5]]\n",
    "    print('\\nLow pole')\n",
    "#     [print(a) for a in sorted(list(zip(model.coef_, feat_names)))[0:5]]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "BUILDING MODEL FOR HEX1_eX_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 0.75%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.71      0.81      0.76      1537\n",
      "       high       0.78      0.86      0.82      1538\n",
      "        low       0.80      0.51      0.62      1003\n",
      "\n",
      "avg / total       0.76      0.75      0.75      4078\n",
      "\n",
      "Accuracy on test sample: 0.53%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.49      0.64      0.56       162\n",
      "       high       0.59      0.70      0.64       166\n",
      "        low       0.43      0.17      0.24       126\n",
      "\n",
      "avg / total       0.51      0.53      0.50       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX2_A_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 0.81%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.79      0.83      0.81      1588\n",
      "       high       0.83      0.80      0.82      1149\n",
      "        low       0.82      0.79      0.80      1341\n",
      "\n",
      "avg / total       0.81      0.81      0.81      4078\n",
      "\n",
      "Accuracy on test sample: 0.60%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.62      0.62      0.62       193\n",
      "       high       0.64      0.63      0.63       124\n",
      "        low       0.52      0.53      0.53       137\n",
      "\n",
      "avg / total       0.60      0.60      0.60       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX3_C_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 0.78%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.85      0.57      0.68      1042\n",
      "       high       0.77      0.86      0.81      1573\n",
      "        low       0.76      0.84      0.80      1463\n",
      "\n",
      "avg / total       0.79      0.78      0.78      4078\n",
      "\n",
      "Accuracy on test sample: 0.54%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.42      0.26      0.32       113\n",
      "       high       0.60      0.64      0.62       187\n",
      "        low       0.51      0.62      0.56       154\n",
      "\n",
      "avg / total       0.52      0.54      0.52       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX4_E_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 0.79%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.84      0.72      0.78      1207\n",
      "       high       0.74      0.95      0.83      1999\n",
      "        low       0.93      0.50      0.65       872\n",
      "\n",
      "avg / total       0.81      0.79      0.78      4078\n",
      "\n",
      "Accuracy on test sample: 0.62%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.62      0.52      0.56       149\n",
      "       high       0.64      0.82      0.72       230\n",
      "        low       0.42      0.20      0.27        75\n",
      "\n",
      "avg / total       0.60      0.62      0.60       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX5_O_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 0.75%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.78      0.52      0.62      1032\n",
      "       high       0.71      0.91      0.80      1949\n",
      "        low       0.82      0.66      0.74      1097\n",
      "\n",
      "avg / total       0.76      0.75      0.74      4078\n",
      "\n",
      "Accuracy on test sample: 0.50%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.34      0.24      0.28       105\n",
      "       high       0.56      0.73      0.63       224\n",
      "        low       0.43      0.30      0.36       125\n",
      "\n",
      "avg / total       0.47      0.50      0.47       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX6_H_nom\n",
      "========================================\n",
      "\n",
      "Accuracy on training sample: 0.77%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.77      0.86      0.81      1710\n",
      "       high       0.95      0.26      0.41       671\n",
      "        low       0.75      0.89      0.81      1697\n",
      "\n",
      "avg / total       0.79      0.77      0.75      4078\n",
      "\n",
      "Accuracy on test sample: 0.58%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.55      0.66      0.60       193\n",
      "       high       0.33      0.03      0.06        65\n",
      "        low       0.62      0.69      0.65       196\n",
      "\n",
      "avg / total       0.55      0.58      0.54       454\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trait in trait_names:\n",
    "    trait = trait+'_nom'\n",
    "    lm = RandomForestClassifier(n_estimators=500, max_features='log2', \n",
    "                                min_samples_leaf=20, oob_score = True)  \n",
    "    lm = LogisticRegression()\n",
    "#     lm = MultinomialNB()\n",
    "#     lm = GradientBoostingClassifier() #seems best as yet\n",
    "    build_model_nom(X_train=X_train, X_test=X_test, \n",
    "                y_train = train.loc[:,trait], y_test = test.loc[:,trait],\n",
    "                vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-815-6aa3642f4bc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     build_model_cont(X_train=X_train, X_test=X_test, \n\u001b[0;32m      7\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrait\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrait\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 vectorizer=vectorizer, model=lm)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-805-f6bfab427e96>\u001b[0m in \u001b[0;36mbuild_model_cont\u001b[1;34m(X_train, X_test, y_train, y_test, vectorizer, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_model_cont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}\\nBUILDING MODEL FOR {}\\n{}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"=\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MAPE on training sample: {:.2f}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 327\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mytas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for trait in trait_names:\n",
    "    lm = RandomForestRegressor(n_estimators=500, max_features='log2', \n",
    "                                min_samples_leaf=12, oob_score = True)  \n",
    "#     lm = LinearRegression()\n",
    "#     lm = GradientBoostingRegressor()\n",
    "    build_model_cont(X_train=X_train, X_test=X_test, \n",
    "                y_train = train.loc[:,trait], y_test = test.loc[:,trait],\n",
    "                vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
