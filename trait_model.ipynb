{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Texts as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out short texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 189.076039088 \n",
      "Median: 61.0 \n",
      "Min: 1 \n",
      "Max: 16384\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "print('Mean:', lens.mean(),\n",
    "      '\\nMedian:', np.median(lens), '\\nMin:', min(lens), '\\nMax:', max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.32% shorter than 700\n",
      "16.88% longer than 200\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 700, 200\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4532, 10)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data = pd.merge(texts, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data.text = data.text.str.lower()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def build_model(data, X, y, vectorizer, model):\n",
    "    print(\"=\"*80)\n",
    "    print('BUILDING MODEL FOR {}'.format(y))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[X], data[y], test_size=0.1)\n",
    "    \n",
    "    print('Train sample: {}\\nTest sample: {}'.format(len(X_train), len(X_test)))\n",
    "        \n",
    "    train_vec = vectorizer.fit_transform(X_train)\n",
    "    test_vec = vectorizer.transform(X_test)\n",
    "     \n",
    "    print('\\nIncluded tokens ({})'.format(train_vec.shape[1]))\n",
    "    print(np.array(vectorizer.get_feature_names())[np.random.randint(0, len(vectorizer.get_feature_names()), 20)])\n",
    "    print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "    print(np.array(list(vectorizer.stop_words_))[np.random.randint(0, len(vectorizer.stop_words_), 20)])\n",
    "    \n",
    "    model.fit(train_vec, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(train_vec)\n",
    "    print('\\nMAPE on training sample: {:.2f}%'.format(mape(y_train, y_train_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_train, y_train_pred)))\n",
    "\n",
    "    y_test_pred = model.predict(test_vec)\n",
    "    print('\\nMAPE on test sample: {:.2f}%'.format(mape(y_test, y_test_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    \n",
    "    print('\\nHigh pole')\n",
    "    #[print(a) for a in sorted(list(zip(model.coef_, vectorizer.get_feature_names())), reverse=True)[0:5]]\n",
    "    print('\\nLow pole')\n",
    "    #[print(a) for a in sorted(list(zip(model.coef_, vectorizer.get_feature_names())))[0:5]]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1123)\n",
      "['других' ', который' 'ей' 'тому' ', из' 'том ,' 'интересно ,'\n",
      " '# цитаты_из_прочитанного' 'кстати' '—' 'вопросы' ', из' '=' 'им' '— это'\n",
      " 'этом' 'мира' ', это' '! ! !' 'свою']\n",
      "\n",
      "Excluded tokens (438187)\n",
      "['не денут ,' 'games' 'балконе' 'них и' 'чистотой' 'виной твоим коленям'\n",
      " ', задавать' 'ценный .' 'инициатива' 'только я' 'в себя ли'\n",
      " 'таким уродцем..' 'ощущений ?' 'и придумали одно' 'что словарь'\n",
      " 'деталях показать процесс' 'заслушалась' 'попала в предпиковую'\n",
      " ', после которой' 'отсутствие и негативность']\n",
      "\n",
      "MAPE on training sample: 6.54%\n",
      "R2 on training sample: 0.872\n",
      "\n",
      "MAPE on test sample: 15.83%\n",
      "R2 on training sample: 0.268\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX2_A\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1128)\n",
      "['моего' 'ибо' 'сильно' 'двух' 'но и' 'сентября' 'словно' 'которым'\n",
      " 'времени' 'минут' 'минут' 'деле' 'одну' '. )' 'знаю' 'точно' 'там'\n",
      " 'человека' 'все ,' 'последний']\n",
      "\n",
      "Excluded tokens (439051)\n",
      "['диагноз . последний' '! ! for' 'случае бездействия аккаунта' 'тараня их'\n",
      " 'пройдите обследование' '. однажды орлов' 'учитель после приветствия'\n",
      " 'доверие .' '. настроение и' 'незакрывающийся' ', ведущая'\n",
      " 'за единую россию' 'очень вашего' 'заканчивается…' 'всемирное наследие »'\n",
      " 'и ресурсам состоялся' 'в качестве примера' ', люблю так' 'а стену'\n",
      " 'разлук . здесь']\n",
      "\n",
      "MAPE on training sample: 6.06%\n",
      "R2 on training sample: 0.873\n",
      "\n",
      "MAPE on test sample: 15.34%\n",
      "R2 on training sample: 0.328\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX3_C\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1120)\n",
      "['... и' 'любовь' 'дней' 'быстро' 'хотя бы' '! и' ', в' 'и так' 'пусть'\n",
      " 'что вы' 'что не' 'чтоб' 'чтоб' 'что в' 'понимаю' 'ответ' 'ж' 'c )'\n",
      " 'еще и' 'том']\n",
      "\n",
      "Excluded tokens (437544)\n",
      "['пушкинский !' 'литры' 'что медики необычайно' 'если на земле'\n",
      " 'населении в 5' 'названием кора' 'леди (' 'несколько книг ...'\n",
      " 'башмаков ,' 'знать , что' 'такая цепочка' 'с непосредственно вживлением'\n",
      " 'в любой отдаленности' 'потребления и ,' 'взломать мою' 'уровень сервиса'\n",
      " 'тебе утром ,' 'кофейного сервиза и' 'в уверенность' 'типа ?']\n",
      "\n",
      "MAPE on training sample: 7.47%\n",
      "R2 on training sample: 0.851\n",
      "\n",
      "MAPE on test sample: 17.94%\n",
      "R2 on training sample: 0.202\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX4_E\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1133)\n",
      "['истории' ', которые' 'ну' 'чему' 'чему' 'на все' 'книги' '! »' 'говорить'\n",
      " 'найти' 'писать' 'и я' 'my' 'бы' 'под' 'многие' 'знаете' '! ``' 'среди'\n",
      " 'тем']\n",
      "\n",
      "Excluded tokens (439078)\n",
      "['? почему ты' 'для тебя слишком' 'реальный шанс поработать'\n",
      " 'женщин при мысли' 'идут , а' 'источником подлинного искусства'\n",
      " 'и монографий' 'forget the sunshine' 'fool in your'\n",
      " 'несоответствие между идеалом' 'us for' 'чернобыль и фукушиму'\n",
      " ', ничего нового' ', more' 'подобрать слова лучше' 'в аптеке :'\n",
      " 'home is parochial' 'confess that your' 'or ,' 'это честь']\n",
      "\n",
      "MAPE on training sample: 6.07%\n",
      "R2 on training sample: 0.863\n",
      "\n",
      "MAPE on test sample: 17.05%\n",
      "R2 on training sample: 0.137\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX5_O\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1132)\n",
      "['и как' 'души' 'человек ,' ', что вы' 'жизни' 'такие' 'искать'\n",
      " 'кажется , что' 'у него' 'хочется' '... и' 'пару' 'как бы' '? )' 'наш'\n",
      " 'зато' 'и ты' 'собой' 'кто-нибудь' '. что']\n",
      "\n",
      "Excluded tokens (438735)\n",
      "['а мальчик рядом' 'нда .' 'твоим дыханием' 'же ребята здорово' 'газельими'\n",
      " 'должны ли' 'гениального от' ': [ id2896530|катринъ' 'креативность . есть'\n",
      " 'silence is' 'того нужно приобрести' 'одного отца' 'лицо полуночное ,'\n",
      " ', как месяца' 'и добавила ,' ', кто подписался' 'определённым критериям'\n",
      " 'полную стоимость' 'составляют предмет' ', паблик']\n",
      "\n",
      "MAPE on training sample: 6.97%\n",
      "R2 on training sample: 0.825\n",
      "\n",
      "MAPE on test sample: 16.92%\n",
      "R2 on training sample: 0.100\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX6_H\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1132)\n",
      "['фото' 'всей' 'при' '. сегодня' ', и я' 'другом' 'случае' 'тех , кто'\n",
      " 'завтра' 'вся' 'этой' 'что если' 'тот' 'сделать' '? )' ', и в' 'смерти'\n",
      " 'больше не' 'последние' 'денег']\n",
      "\n",
      "Excluded tokens (438931)\n",
      "['улице такая плохая' 'было плевать' 'играйте' ': ул' 'позже ,'\n",
      " ', интересная' ') . возвращаюсь' 'в качестве научрука' 'бену'\n",
      " 'южноуральские' 'снежным городом' 'тянет .' 'крепко спим' 'harris'\n",
      " 'вас есть братья' 'и хочется подойти' 'умней' 'биологию на'\n",
      " 'жизнеутверждающие (' 'враги не..']\n",
      "\n",
      "MAPE on training sample: 6.75%\n",
      "R2 on training sample: 0.858\n",
      "\n",
      "MAPE on test sample: 16.07%\n",
      "R2 on training sample: 0.302\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    lm = RandomForestRegressor()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 30, \n",
    "                         max_df = 0.7, \n",
    "                         max_features = 10000)\n",
    "    build_model(data, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - concatenated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 2)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts['text'] = texts['text'].apply(str).apply(str.lower)\n",
    "texts_conc = texts.groupby('owner_id')['text'].apply(lambda x: ' <ps> '.join(x))\n",
    "texts_conc = pd.DataFrame(texts_conc.reset_index())\n",
    "texts_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 70617.0566038 \n",
      "Median: 26465.5 \n",
      "Min: 85 \n",
      "Max: 918554\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts_conc.text])\n",
    "print('Mean:', lens.mean(),\n",
    "      '\\nMedian:', np.median(lens), '\\nMin:', min(lens), '\\nMax:', max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% shorter than 10000000000\n",
      "95.28% longer than 1000\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 10**10, 1000\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts_conc.text])\n",
    "texts_conc = texts_conc[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 10)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data2 = pd.merge(texts_conc, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['него' 'невозможно' 'решил' 'that' 'целый' 'хороший' 'фото' 'кроме'\n",
      " 'мнение' 'благодаря' 'мозг' 'дом' 'хотя' 'сначала' 'почти' 'сторону'\n",
      " 'постоянно' 'цветы' 'могли' 'говорят']\n",
      "\n",
      "Excluded tokens (133131)\n",
      "['бить' 'ничтожного' 'знакомо-то' 'саше' 'растаман' 'впечатляет' 'соседняя'\n",
      " 'аргентину' 'сашулей' 'скрытая' 'знакомыми' 'анализировали' 'просыпается'\n",
      " 'универсистеского' 'налил' 'гораций' 'сравнивала' 'dumb' 'клан' 'среднюю']\n",
      "\n",
      "MAPE on training sample: 6.48%\n",
      "R2 on training sample: 0.833\n",
      "\n",
      "MAPE on test sample: 16.32%\n",
      "R2 on training sample: 0.324\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX2_A\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['so' 'вконтакте' 'хорошего' 'like' 'прекрасный' 'результаты' 'наконец-то'\n",
      " 'группы' '***' 'всеми' '@' 'кого-то' 'об' 'кажется' 'делают' 'xd'\n",
      " 'настроение' 'днем' 'наше' 'ветер']\n",
      "\n",
      "Excluded tokens (109938)\n",
      "['погибает' 'альтернативный' 'ярослав' 'замело' 'блогерам' 'олимпиаду'\n",
      " 'зверей' 'луций' 'поговори' 'dreams' 'понта' 'стук-стук' 'сумерки'\n",
      " 'freebsd' 'внешностью' 'комета' 'kalos' 'поищите' 'силлабус' 'тудентов']\n",
      "\n",
      "MAPE on training sample: 6.43%\n",
      "R2 on training sample: 0.838\n",
      "\n",
      "MAPE on test sample: 15.64%\n",
      "R2 on training sample: 0.244\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX3_C\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['глазах' 'ищу' 'was' 'конца' 'другой' 'каждого' 'd0' 'наоборот' 'самые'\n",
      " 'своим' 'прошу' 'живут' 'сердца' 'месте' 'говоря' 'мама' 'радость' 'ночи'\n",
      " '6.' 'be']\n",
      "\n",
      "Excluded tokens (131813)\n",
      "['нобелевка' 'обсуждении' 'тикси' '18-00' 'сохраните' 'платно.'\n",
      " 'промывания' 'вечна' 'привязано' 'закивал' 'безмолвное' 'перебиваются'\n",
      " 'необычайной' 'нанотехнологией' 'touchent'\n",
      " '//bulalex.ru/2011-03-21-16-31-18/roomesc' 'прaвды' 'порекомендую'\n",
      " 'пресли' 'глазном']\n",
      "\n",
      "MAPE on training sample: 6.73%\n",
      "R2 on training sample: 0.844\n",
      "\n",
      "MAPE on test sample: 28.22%\n",
      "R2 on training sample: 0.079\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX4_E\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['радости' 'хватит' '1.' 'любой' 'just' 'месяца' 'новые' 'одну' 'ночью'\n",
      " 'решил' 'месте' 'почти' 'часть' 'наконец-то' 'других' 'ко' 'россия'\n",
      " 'долго' 'недели' 'встречи']\n",
      "\n",
      "Excluded tokens (125321)\n",
      "['кінця' 'радужных' '*плачу*' 'главнейшим' 'halen' '😎💵💸💡' 'марьиванна'\n",
      " 'плодотворно' 'всяческие' 'ценить' 'пояаилось' 'документалка' 'моральны'\n",
      " 'обитают' 'военным' 'вынули' 'трояка' 'шатким' '|элиезер'\n",
      " 'сонечкой-макаронечкой']\n",
      "\n",
      "MAPE on training sample: 7.47%\n",
      "R2 on training sample: 0.768\n",
      "\n",
      "MAPE on test sample: 26.54%\n",
      "R2 on training sample: -0.105\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX5_O\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['маленький' 'd0' 'обычно' 'далеко' 'ах' 'd1' 'работу' 'дверь' 'месте' 'ко'\n",
      " 'последние' 'ответ' 'know' 'прям' 'языка' 'нужен' 'другого' 'самого'\n",
      " 'общем' '4.']\n",
      "\n",
      "Excluded tokens (117301)\n",
      "['карлом' 'музейный' 'корила' 'лекаря' 'выла' 'штанишках' 'радуемсо'\n",
      " 'канатики' 'оказания' 'гущи' 'помята' 'спектор' 'остально' 'клюшкой'\n",
      " 'зазываю' 'рабочим' 'извилина' 'подоспело' 'заберёт' 'id234517589|тим']\n",
      "\n",
      "MAPE on training sample: 7.34%\n",
      "R2 on training sample: 0.790\n",
      "\n",
      "MAPE on test sample: 15.01%\n",
      "R2 on training sample: -0.326\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX6_H\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['сами' 'чай' '50' 'кем' '1.' 'полностью' 'кг' 'плохо' 'большой' 'любить'\n",
      " 'домой' 'обожаю' 'далеко' 'видно' 'интересно' 'вся' 'знать' 'выглядит'\n",
      " 'воды' 'готова']\n",
      "\n",
      "Excluded tokens (124511)\n",
      "['еженедельных' 'искусствовед' 'тойча' 'пьесу' 'роллс-ройса' 'известны'\n",
      " 'шлюху' 'удивились' 'inspiring' 'батлер' 'копила' 'глянцевом' 'соседская'\n",
      " 'прихватил' 'схлопываешь' 'личеый' 'по-старому' 'гляжусь' 'балдела'\n",
      " 'cerises']\n",
      "\n",
      "MAPE on training sample: 7.93%\n",
      "R2 on training sample: 0.806\n",
      "\n",
      "MAPE on test sample: 18.72%\n",
      "R2 on training sample: -0.098\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    lm = RandomForestRegressor()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 10, \n",
    "                         max_df = 0.7, \n",
    "                         max_features = 1000)\n",
    "    build_model(data2, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
