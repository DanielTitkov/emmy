{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Texts as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out short texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 189.076039088 \n",
      "Median: 61.0 \n",
      "Min: 1 \n",
      "Max: 16384\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "print('Mean:', lens.mean(),\n",
    "      '\\nMedian:', np.median(lens), '\\nMin:', min(lens), '\\nMax:', max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.32% shorter than 700\n",
      "16.88% longer than 200\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 700, 200\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4532, 10)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data = pd.merge(texts, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data.text = data.text.str.lower()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def build_model(data, X, y, vectorizer, model):\n",
    "    print(\"=\"*80)\n",
    "    print('BUILDING MODEL FOR {}'.format(y))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[X], data[y], test_size=0.1)\n",
    "    \n",
    "    print('Train sample: {}\\nTest sample: {}'.format(len(X_train), len(X_test)))\n",
    "        \n",
    "    train_vec = vectorizer.fit_transform(X_train)\n",
    "    test_vec = vectorizer.transform(X_test)\n",
    "     \n",
    "    print('\\nIncluded tokens ({})'.format(train_vec.shape[1]))\n",
    "    print(np.array(vectorizer.get_feature_names())[np.random.randint(0, len(vectorizer.get_feature_names()), 20)])\n",
    "    print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "    print(np.array(list(vectorizer.stop_words_))[np.random.randint(0, len(vectorizer.stop_words_), 20)])\n",
    "    \n",
    "    model.fit(train_vec, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(train_vec)\n",
    "    print('\\nMAPE on training sample: {:.2f}%'.format(mape(y_train, y_train_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_train, y_train_pred)))\n",
    "\n",
    "    y_test_pred = model.predict(test_vec)\n",
    "    print('\\nMAPE on test sample: {:.2f}%'.format(mape(y_test, y_test_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    \n",
    "    print('\\nHigh pole')\n",
    "    #[print(a) for a in sorted(list(zip(model.coef_, vectorizer.get_feature_names())), reverse=True)[0:5]]\n",
    "    print('\\nLow pole')\n",
    "    #[print(a) for a in sorted(list(zip(model.coef_, vectorizer.get_feature_names())))[0:5]]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1123)\n",
      "['других' ', который' 'ей' 'тому' ', из' 'том ,' 'интересно ,'\n",
      " '# цитаты_из_прочитанного' 'кстати' '—' 'вопросы' ', из' '=' 'им' '— это'\n",
      " 'этом' 'мира' ', это' '! ! !' 'свою']\n",
      "\n",
      "Excluded tokens (438187)\n",
      "['не денут ,' 'games' 'балконе' 'них и' 'чистотой' 'виной твоим коленям'\n",
      " ', задавать' 'ценный .' 'инициатива' 'только я' 'в себя ли'\n",
      " 'таким уродцем..' 'ощущений ?' 'и придумали одно' 'что словарь'\n",
      " 'деталях показать процесс' 'заслушалась' 'попала в предпиковую'\n",
      " ', после которой' 'отсутствие и негативность']\n",
      "\n",
      "MAPE on training sample: 6.54%\n",
      "R2 on training sample: 0.872\n",
      "\n",
      "MAPE on test sample: 15.83%\n",
      "R2 on training sample: 0.268\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX2_A\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1128)\n",
      "['моего' 'ибо' 'сильно' 'двух' 'но и' 'сентября' 'словно' 'которым'\n",
      " 'времени' 'минут' 'минут' 'деле' 'одну' '. )' 'знаю' 'точно' 'там'\n",
      " 'человека' 'все ,' 'последний']\n",
      "\n",
      "Excluded tokens (439051)\n",
      "['диагноз . последний' '! ! for' 'случае бездействия аккаунта' 'тараня их'\n",
      " 'пройдите обследование' '. однажды орлов' 'учитель после приветствия'\n",
      " 'доверие .' '. настроение и' 'незакрывающийся' ', ведущая'\n",
      " 'за единую россию' 'очень вашего' 'заканчивается…' 'всемирное наследие »'\n",
      " 'и ресурсам состоялся' 'в качестве примера' ', люблю так' 'а стену'\n",
      " 'разлук . здесь']\n",
      "\n",
      "MAPE on training sample: 6.06%\n",
      "R2 on training sample: 0.873\n",
      "\n",
      "MAPE on test sample: 15.34%\n",
      "R2 on training sample: 0.328\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX3_C\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1120)\n",
      "['... и' 'любовь' 'дней' 'быстро' 'хотя бы' '! и' ', в' 'и так' 'пусть'\n",
      " 'что вы' 'что не' 'чтоб' 'чтоб' 'что в' 'понимаю' 'ответ' 'ж' 'c )'\n",
      " 'еще и' 'том']\n",
      "\n",
      "Excluded tokens (437544)\n",
      "['пушкинский !' 'литры' 'что медики необычайно' 'если на земле'\n",
      " 'населении в 5' 'названием кора' 'леди (' 'несколько книг ...'\n",
      " 'башмаков ,' 'знать , что' 'такая цепочка' 'с непосредственно вживлением'\n",
      " 'в любой отдаленности' 'потребления и ,' 'взломать мою' 'уровень сервиса'\n",
      " 'тебе утром ,' 'кофейного сервиза и' 'в уверенность' 'типа ?']\n",
      "\n",
      "MAPE on training sample: 7.47%\n",
      "R2 on training sample: 0.851\n",
      "\n",
      "MAPE on test sample: 17.94%\n",
      "R2 on training sample: 0.202\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX4_E\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1133)\n",
      "['истории' ', которые' 'ну' 'чему' 'чему' 'на все' 'книги' '! »' 'говорить'\n",
      " 'найти' 'писать' 'и я' 'my' 'бы' 'под' 'многие' 'знаете' '! ``' 'среди'\n",
      " 'тем']\n",
      "\n",
      "Excluded tokens (439078)\n",
      "['? почему ты' 'для тебя слишком' 'реальный шанс поработать'\n",
      " 'женщин при мысли' 'идут , а' 'источником подлинного искусства'\n",
      " 'и монографий' 'forget the sunshine' 'fool in your'\n",
      " 'несоответствие между идеалом' 'us for' 'чернобыль и фукушиму'\n",
      " ', ничего нового' ', more' 'подобрать слова лучше' 'в аптеке :'\n",
      " 'home is parochial' 'confess that your' 'or ,' 'это честь']\n",
      "\n",
      "MAPE on training sample: 6.07%\n",
      "R2 on training sample: 0.863\n",
      "\n",
      "MAPE on test sample: 17.05%\n",
      "R2 on training sample: 0.137\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX5_O\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1132)\n",
      "['и как' 'души' 'человек ,' ', что вы' 'жизни' 'такие' 'искать'\n",
      " 'кажется , что' 'у него' 'хочется' '... и' 'пару' 'как бы' '? )' 'наш'\n",
      " 'зато' 'и ты' 'собой' 'кто-нибудь' '. что']\n",
      "\n",
      "Excluded tokens (438735)\n",
      "['а мальчик рядом' 'нда .' 'твоим дыханием' 'же ребята здорово' 'газельими'\n",
      " 'должны ли' 'гениального от' ': [ id2896530|катринъ' 'креативность . есть'\n",
      " 'silence is' 'того нужно приобрести' 'одного отца' 'лицо полуночное ,'\n",
      " ', как месяца' 'и добавила ,' ', кто подписался' 'определённым критериям'\n",
      " 'полную стоимость' 'составляют предмет' ', паблик']\n",
      "\n",
      "MAPE on training sample: 6.97%\n",
      "R2 on training sample: 0.825\n",
      "\n",
      "MAPE on test sample: 16.92%\n",
      "R2 on training sample: 0.100\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX6_H\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1132)\n",
      "['фото' 'всей' 'при' '. сегодня' ', и я' 'другом' 'случае' 'тех , кто'\n",
      " 'завтра' 'вся' 'этой' 'что если' 'тот' 'сделать' '? )' ', и в' 'смерти'\n",
      " 'больше не' 'последние' 'денег']\n",
      "\n",
      "Excluded tokens (438931)\n",
      "['улице такая плохая' 'было плевать' 'играйте' ': ул' 'позже ,'\n",
      " ', интересная' ') . возвращаюсь' 'в качестве научрука' 'бену'\n",
      " 'южноуральские' 'снежным городом' 'тянет .' 'крепко спим' 'harris'\n",
      " 'вас есть братья' 'и хочется подойти' 'умней' 'биологию на'\n",
      " 'жизнеутверждающие (' 'враги не..']\n",
      "\n",
      "MAPE on training sample: 6.75%\n",
      "R2 on training sample: 0.858\n",
      "\n",
      "MAPE on test sample: 16.07%\n",
      "R2 on training sample: 0.302\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    lm = RandomForestRegressor()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 30, \n",
    "                         max_df = 0.7, \n",
    "                         max_features = 10000)\n",
    "    build_model(data, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - concatenated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 2)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts['text'] = texts['text'].apply(str).apply(str.lower)\n",
    "texts_conc = texts.groupby('owner_id')['text'].apply(lambda x: ' <ps> '.join(x))\n",
    "texts_conc = pd.DataFrame(texts_conc.reset_index())\n",
    "texts_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 70617.0566038 \n",
      "Median: 26465.5 \n",
      "Min: 85 \n",
      "Max: 918554\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts_conc.text])\n",
    "print('Mean:', lens.mean(),\n",
    "      '\\nMedian:', np.median(lens), '\\nMin:', min(lens), '\\nMax:', max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% shorter than 10000000000\n",
      "95.28% longer than 1000\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 10**10, 1000\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts_conc.text])\n",
    "texts_conc = texts_conc[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 10)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data2 = pd.merge(texts_conc, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['него' 'невозможно' 'решил' 'that' 'целый' 'хороший' 'фото' 'кроме'\n",
      " 'мнение' 'благодаря' 'мозг' 'дом' 'хотя' 'сначала' 'почти' 'сторону'\n",
      " 'постоянно' 'цветы' 'могли' 'говорят']\n",
      "\n",
      "Excluded tokens (133131)\n",
      "['бить' 'ничтожного' 'знакомо-то' 'саше' 'растаман' 'впечатляет' 'соседняя'\n",
      " 'аргентину' 'сашулей' 'скрытая' 'знакомыми' 'анализировали' 'просыпается'\n",
      " 'универсистеского' 'налил' 'гораций' 'сравнивала' 'dumb' 'клан' 'среднюю']\n",
      "\n",
      "MAPE on training sample: 6.48%\n",
      "R2 on training sample: 0.833\n",
      "\n",
      "MAPE on test sample: 16.32%\n",
      "R2 on training sample: 0.324\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX2_A\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['so' 'вконтакте' 'хорошего' 'like' 'прекрасный' 'результаты' 'наконец-то'\n",
      " 'группы' '***' 'всеми' '@' 'кого-то' 'об' 'кажется' 'делают' 'xd'\n",
      " 'настроение' 'днем' 'наше' 'ветер']\n",
      "\n",
      "Excluded tokens (109938)\n",
      "['погибает' 'альтернативный' 'ярослав' 'замело' 'блогерам' 'олимпиаду'\n",
      " 'зверей' 'луций' 'поговори' 'dreams' 'понта' 'стук-стук' 'сумерки'\n",
      " 'freebsd' 'внешностью' 'комета' 'kalos' 'поищите' 'силлабус' 'тудентов']\n",
      "\n",
      "MAPE on training sample: 6.43%\n",
      "R2 on training sample: 0.838\n",
      "\n",
      "MAPE on test sample: 15.64%\n",
      "R2 on training sample: 0.244\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX3_C\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['глазах' 'ищу' 'was' 'конца' 'другой' 'каждого' 'd0' 'наоборот' 'самые'\n",
      " 'своим' 'прошу' 'живут' 'сердца' 'месте' 'говоря' 'мама' 'радость' 'ночи'\n",
      " '6.' 'be']\n",
      "\n",
      "Excluded tokens (131813)\n",
      "['нобелевка' 'обсуждении' 'тикси' '18-00' 'сохраните' 'платно.'\n",
      " 'промывания' 'вечна' 'привязано' 'закивал' 'безмолвное' 'перебиваются'\n",
      " 'необычайной' 'нанотехнологией' 'touchent'\n",
      " '//bulalex.ru/2011-03-21-16-31-18/roomesc' 'прaвды' 'порекомендую'\n",
      " 'пресли' 'глазном']\n",
      "\n",
      "MAPE on training sample: 6.73%\n",
      "R2 on training sample: 0.844\n",
      "\n",
      "MAPE on test sample: 28.22%\n",
      "R2 on training sample: 0.079\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX4_E\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['радости' 'хватит' '1.' 'любой' 'just' 'месяца' 'новые' 'одну' 'ночью'\n",
      " 'решил' 'месте' 'почти' 'часть' 'наконец-то' 'других' 'ко' 'россия'\n",
      " 'долго' 'недели' 'встречи']\n",
      "\n",
      "Excluded tokens (125321)\n",
      "['кінця' 'радужных' '*плачу*' 'главнейшим' 'halen' '😎💵💸💡' 'марьиванна'\n",
      " 'плодотворно' 'всяческие' 'ценить' 'пояаилось' 'документалка' 'моральны'\n",
      " 'обитают' 'военным' 'вынули' 'трояка' 'шатким' '|элиезер'\n",
      " 'сонечкой-макаронечкой']\n",
      "\n",
      "MAPE on training sample: 7.47%\n",
      "R2 on training sample: 0.768\n",
      "\n",
      "MAPE on test sample: 26.54%\n",
      "R2 on training sample: -0.105\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX5_O\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['маленький' 'd0' 'обычно' 'далеко' 'ах' 'd1' 'работу' 'дверь' 'месте' 'ко'\n",
      " 'последние' 'ответ' 'know' 'прям' 'языка' 'нужен' 'другого' 'самого'\n",
      " 'общем' '4.']\n",
      "\n",
      "Excluded tokens (117301)\n",
      "['карлом' 'музейный' 'корила' 'лекаря' 'выла' 'штанишках' 'радуемсо'\n",
      " 'канатики' 'оказания' 'гущи' 'помята' 'спектор' 'остально' 'клюшкой'\n",
      " 'зазываю' 'рабочим' 'извилина' 'подоспело' 'заберёт' 'id234517589|тим']\n",
      "\n",
      "MAPE on training sample: 7.34%\n",
      "R2 on training sample: 0.790\n",
      "\n",
      "MAPE on test sample: 15.01%\n",
      "R2 on training sample: -0.326\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX6_H\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['сами' 'чай' '50' 'кем' '1.' 'полностью' 'кг' 'плохо' 'большой' 'любить'\n",
      " 'домой' 'обожаю' 'далеко' 'видно' 'интересно' 'вся' 'знать' 'выглядит'\n",
      " 'воды' 'готова']\n",
      "\n",
      "Excluded tokens (124511)\n",
      "['еженедельных' 'искусствовед' 'тойча' 'пьесу' 'роллс-ройса' 'известны'\n",
      " 'шлюху' 'удивились' 'inspiring' 'батлер' 'копила' 'глянцевом' 'соседская'\n",
      " 'прихватил' 'схлопываешь' 'личеый' 'по-старому' 'гляжусь' 'балдела'\n",
      " 'cerises']\n",
      "\n",
      "MAPE on training sample: 7.93%\n",
      "R2 on training sample: 0.806\n",
      "\n",
      "MAPE on test sample: 18.72%\n",
      "R2 on training sample: -0.098\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    lm = RandomForestRegressor()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 10, \n",
    "                         max_df = 0.7, \n",
    "                         max_features = 1000)\n",
    "    build_model(data2, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Nominal traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_groups(x, dev=1, M=50, SD=10):\n",
    "    if x > M+dev*SD:\n",
    "        return 'high'\n",
    "    elif x < M-dev*SD:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEX1_eX\n",
      "high       53\n",
      "average    51\n",
      "low        48\n",
      "Name: HEX1_eX_nom, dtype: int64\n",
      "HEX2_A\n",
      "average    58\n",
      "high       51\n",
      "low        43\n",
      "Name: HEX2_A_nom, dtype: int64\n",
      "HEX3_C\n",
      "average    53\n",
      "high       52\n",
      "low        47\n",
      "Name: HEX3_C_nom, dtype: int64\n",
      "HEX4_E\n",
      "high       57\n",
      "average    48\n",
      "low        47\n",
      "Name: HEX4_E_nom, dtype: int64\n",
      "HEX5_O\n",
      "high       56\n",
      "average    50\n",
      "low        46\n",
      "Name: HEX5_O_nom, dtype: int64\n",
      "HEX6_H\n",
      "average    54\n",
      "high       50\n",
      "low        48\n",
      "Name: HEX6_H_nom, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    scale = trait + '_nom'\n",
    "    traits[scale] = traits[trait].apply(set_groups, dev=0.5)\n",
    "    print(trait)\n",
    "    print(traits[scale].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.32% shorter than 700\n",
      "16.88% longer than 200\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 700, 200\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(s):\n",
    "    rgxp = '[\\`\\)\\(\\|©~^<>/\\'\\\"\\«№#$&\\*.,;=+?!\\—_@:\\]\\[%\\{\\}\\\\n]'\n",
    "    return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     мастер ты говорил что если я познаю кто я то ...\n",
       "1    однажды солдат охранявший дорогу остановил буд...\n",
       "2    люди я хочу извиниться перед вами…перед всеми ...\n",
       "3    what i plainly see before my eyes makes me fin...\n",
       "4    здравствуй мальчик бананан если ты нормальный ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data3 = pd.merge(texts, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data3.text = data3.text.str.lower().apply(cleanse)\n",
    "data3.shape\n",
    "data3.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(data, X, y, vectorizer, model):\n",
    "    print(\"=\"*40)\n",
    "    print('BUILDING MODEL FOR {}'.format(y))\n",
    "    print(\"=\"*40)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[X], data[y], test_size=0.1)\n",
    "    print('Train sample: {}\\nTest sample: {}'.format(len(X_train), len(X_test)))\n",
    "    train_vec = vectorizer.fit_transform(X_train)\n",
    "    test_vec = vectorizer.transform(X_test)\n",
    "    print('\\nIncluded tokens ({})'.format(train_vec.shape[1]))\n",
    "    print(np.array(vectorizer.get_feature_names())[np.random.randint(0, len(vectorizer.get_feature_names()), 20)])\n",
    "    print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "    print(np.array(list(vectorizer.stop_words_))[np.random.randint(0, len(vectorizer.stop_words_), 20)])\n",
    "    model.fit(train_vec, y_train)\n",
    "    y_train_pred = model.predict(train_vec)\n",
    "    print('\\nAccuracy on training sample: {:.2f}%'.format(accuracy_score(y_train, y_train_pred)))\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    y_test_pred = model.predict(test_vec)\n",
    "    print('Accuracy on test sample: {:.2f}%'.format(accuracy_score(y_test, y_test_pred)))\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "BUILDING MODEL FOR HEX1_eX_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (832)\n",
      "['важно' 'слишком' 'будто' 'раз' 'кого' 'is' 'душе' 'оказывается' 'лет'\n",
      " 'не было' 'дела' 'перед' 'даже' 'но не' 'взгляд' 'очень' 'так как' 'себе'\n",
      " 'стороны' 'в том']\n",
      "\n",
      "Excluded tokens (398142)\n",
      "['презентацию' 'кое-какими' 'насладиться этим помедитировать'\n",
      " 'буду писать о' 'войной и' 'отливов' 'то приходится' 'студентом и не'\n",
      " 'сравнивают положение своих' 'накурено очень' 'из них видимо'\n",
      " 'порывистого мира' 'могу быть там' 'p p s' 'id1775145 кирилла'\n",
      " 'пара штучек лишних' 'твиттере очень' 'obshem minutka tragizma'\n",
      " 'или читать' 'тратьте плюсы у']\n",
      "\n",
      "Accuracy on training sample: 0.64%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.61      0.71      0.66      1536\n",
      "       high       0.66      0.76      0.70      1516\n",
      "        low       0.66      0.35      0.46      1026\n",
      "\n",
      "avg / total       0.64      0.64      0.63      4078\n",
      "\n",
      "Accuracy on test sample: 0.55%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.49      0.60      0.54       163\n",
      "       high       0.62      0.68      0.65       188\n",
      "        low       0.46      0.22      0.30       103\n",
      "\n",
      "avg / total       0.54      0.55      0.53       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX2_A_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (850)\n",
      "['хочешь' 'настолько' 'лето' 'него' 'домой' 'мечты' 'далее' 'совсем' 'мир'\n",
      " 'после' 'ощущение' 'нового' 'стать' 'вдруг' 'никогда не' 'лично' 'каждого'\n",
      " 'com' 'но в' 'не надо']\n",
      "\n",
      "Excluded tokens (399254)\n",
      "['и кой' 'кафедра' 'четыре года уже' 'как участник'\n",
      " 'реализации понимаешь что' 'зарево красна вдовою' 'общем мне он'\n",
      " 'утёкшим в' 'в тоске' 'альбомы в' 'друга осуществляйте ваши'\n",
      " 'жизнь была больше' 'очень важную' 'е мексика' 'и эту черную'\n",
      " 'центре таких вот' 'everypony' 'пропустили чем там'\n",
      " 'размеренная и по-своему' 'пидарасы пидарасы пидарасы']\n",
      "\n",
      "Accuracy on training sample: 0.70%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.67      0.75      0.71      1615\n",
      "       high       0.75      0.72      0.73      1138\n",
      "        low       0.70      0.63      0.66      1325\n",
      "\n",
      "avg / total       0.70      0.70      0.70      4078\n",
      "\n",
      "Accuracy on test sample: 0.53%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.47      0.62      0.54       166\n",
      "       high       0.62      0.50      0.56       135\n",
      "        low       0.54      0.45      0.49       153\n",
      "\n",
      "avg / total       0.54      0.53      0.53       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX3_C_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (850)\n",
      "['николай' 'прямо' 'конца' 'можете' 'потому что' 'два' 'сильно' 'из-за'\n",
      " 'мою' 'самая' 'вас' 'постоянно' 'большое' 'ночью' 'нравится' 'in' 'еще и'\n",
      " 'быстро' 'я не' 'не могу']\n",
      "\n",
      "Excluded tokens (398446)\n",
      "['they can change' 'house for the' 'на официальном сайте' 'он плавает'\n",
      " 'цветы фотосессия' 'неизвестная группа выступает' 'москве-то половина'\n",
      " 'это чудо если' 'полностью не убедится' 'я сопереживаю сколько' 'жалко 😇'\n",
      " 'живут долго' 'и очень хорошо' 'под дудочку' 'жили дивные'\n",
      " 'камчатка прекрасна и' 'xxx однажды я' 'сигарет но не' 'через все страхи'\n",
      " 'очень непростым было']\n",
      "\n",
      "Accuracy on training sample: 0.66%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.74      0.38      0.50      1044\n",
      "       high       0.65      0.79      0.71      1599\n",
      "        low       0.65      0.72      0.68      1435\n",
      "\n",
      "avg / total       0.67      0.66      0.65      4078\n",
      "\n",
      "Accuracy on test sample: 0.53%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.35      0.15      0.21       111\n",
      "       high       0.54      0.68      0.60       161\n",
      "        low       0.56      0.62      0.59       182\n",
      "\n",
      "avg / total       0.50      0.53      0.50       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX4_E_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (860)\n",
      "['знаю' 'общем' 'чуть' 'моим' 'ж' 'и т' '4' 'нужен' 'https' 'если' 'мир'\n",
      " 'писать' 'мысли' '2' 'ей' 'хорошо' 'и не' 'плохо' 'никто' 'а в']\n",
      "\n",
      "Excluded tokens (399965)\n",
      "['стк' 'этим ничего' 'афишке сериала' 'полей с 8' 'холодным летним днем'\n",
      " 'свой счет очень' 'остальное сделают хорошая' 'этому фильму иными'\n",
      " 'операцию но' 'слушать дышать' 'ограничивает доступ'\n",
      " 'результате уровень натрия' 'кассе старбакса' 'ведется на это'\n",
      " 'храм zenkojidani' 'мужественные умные и' 'и террористической по'\n",
      " 'озвучить ему' 'что-то полностью' 'там рос']\n",
      "\n",
      "Accuracy on training sample: 0.68%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.72      0.61      0.66      1225\n",
      "       high       0.66      0.91      0.76      1990\n",
      "        low       0.79      0.26      0.39       863\n",
      "\n",
      "avg / total       0.70      0.68      0.65      4078\n",
      "\n",
      "Accuracy on test sample: 0.63%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.60      0.47      0.52       131\n",
      "       high       0.64      0.87      0.74       239\n",
      "        low       0.60      0.21      0.32        84\n",
      "\n",
      "avg / total       0.62      0.63      0.60       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX5_O_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (844)\n",
      "['эти' 'этой' 'где' 'репост' 'новый' 'я буду' 'в россии' 'мысль' 'мечты'\n",
      " 'иногда' 'мы' 'метро' 'словно' 'даже не' 'её' 'котором' 'стать' 'работа'\n",
      " 'на все' 'быстро']\n",
      "\n",
      "Excluded tokens (398377)\n",
      "['то отдельный' 'теперь уже руководитель' 'лота' 'да наверное'\n",
      " 'надо делать скидку' '04 03 vzryv-v-peterburgskom-metro-hronika' 'формата'\n",
      " 'живу где-то на' 'чего может довести' 'не забудь репостнул'\n",
      " 'пакетами чему вы' 'меня глядели' 'и так минут' '30 ч' 'хочется взять и'\n",
      " 'натуру' 'жеребце' 'выхи' 'мягкого разомлевшего начинаю'\n",
      " 'исключений источник http']\n",
      "\n",
      "Accuracy on training sample: 0.63%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.65      0.29      0.41      1013\n",
      "       high       0.61      0.89      0.72      1963\n",
      "        low       0.70      0.48      0.57      1102\n",
      "\n",
      "avg / total       0.64      0.63      0.60      4078\n",
      "\n",
      "Accuracy on test sample: 0.52%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.40      0.15      0.22       124\n",
      "       high       0.54      0.80      0.65       210\n",
      "        low       0.49      0.38      0.43       120\n",
      "\n",
      "avg / total       0.49      0.52      0.47       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX6_H_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (841)\n",
      "['другого' 'все что' 'читать' 'be' 'кто не' 'человека' 'florist' 'начала'\n",
      " 'это было' 'тебе' 'перед' 'образом' 'собой' 'т д' 'in' '- а' 'видел'\n",
      " 'ними' 'человеку' 'из']\n",
      "\n",
      "Excluded tokens (397947)\n",
      "['и читаются' 'он делает не' 'в отчетике' 'recognize' 'вас нарастает'\n",
      " 'мысль да что' 'отсутствие напряжения конфуций' '6000 рублей' 'оставляю'\n",
      " 'симпатичен но дополнительное' 'и пестреют' 'с привязанностью нельзя'\n",
      " 'правда только в' 'прошел зодиакальный' 'что замерзшая река'\n",
      " 'неповторимые движения дэйва' 'людмила огородова' 'точно будет'\n",
      " 'извиняюсь но далее' 'встретишь в']\n",
      "\n",
      "Accuracy on training sample: 0.66%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.66      0.75      0.70      1699\n",
      "       high       0.83      0.07      0.12       662\n",
      "        low       0.65      0.79      0.71      1717\n",
      "\n",
      "avg / total       0.68      0.66      0.61      4078\n",
      "\n",
      "Accuracy on test sample: 0.59%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.63      0.71      0.66       204\n",
      "       high       0.50      0.04      0.07        74\n",
      "        low       0.56      0.69      0.62       176\n",
      "\n",
      "avg / total       0.58      0.59      0.55       454\n",
      "\n",
      "\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX_nom', 'HEX2_A_nom', 'HEX3_C_nom', 'HEX4_E_nom', 'HEX5_O_nom', 'HEX6_H_nom']:\n",
    "    lm = RandomForestClassifier(n_estimators=500, max_features='log2', \n",
    "                                min_samples_leaf=20, oob_score = True)  \n",
    "    lm = LogisticRegression()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 30, \n",
    "                         max_df = 0.3, \n",
    "                         max_features = 10000)\n",
    "    build_model2(data3, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Naive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we achieved anything at all? Are our models even \"napolshischechki\" better than naive model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NAIVE MODEL FOR HEX1_eX_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.35%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.39      0.42      0.40      1699\n",
      "       high       0.39      0.32      0.35      1704\n",
      "        low       0.25      0.30      0.28      1129\n",
      "\n",
      "avg / total       0.36      0.35      0.35      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX2_A_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.35%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.40      0.40      0.40      1781\n",
      "       high       0.29      0.32      0.31      1273\n",
      "        low       0.34      0.30      0.32      1478\n",
      "\n",
      "avg / total       0.35      0.35      0.35      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX3_C_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.26      0.41      0.32      1155\n",
      "       high       0.40      0.32      0.35      1760\n",
      "        low       0.36      0.30      0.33      1617\n",
      "\n",
      "avg / total       0.35      0.33      0.34      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX4_E_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.32%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.28      0.37      0.32      1356\n",
      "       high       0.48      0.30      0.37      2229\n",
      "        low       0.20      0.29      0.24       947\n",
      "\n",
      "avg / total       0.36      0.32      0.33      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX5_O_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.26      0.41      0.32      1137\n",
      "       high       0.48      0.31      0.38      2173\n",
      "        low       0.27      0.29      0.28      1222\n",
      "\n",
      "avg / total       0.37      0.33      0.34      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX6_H_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.41      0.39      0.40      1903\n",
      "       high       0.16      0.30      0.21       736\n",
      "        low       0.39      0.28      0.33      1893\n",
      "\n",
      "avg / total       0.36      0.33      0.34      4532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_naive = np.random.choice(['high', 'average', 'low'], size=(len(data3),), p=[0.3, 0.4, 0.3])\n",
    "for trait in ['HEX1_eX_nom', 'HEX2_A_nom', 'HEX3_C_nom', 'HEX4_E_nom', 'HEX5_O_nom', 'HEX6_H_nom']:\n",
    "    print(\"=\"*80)\n",
    "    print('NAIVE MODEL FOR {}'.format(trait))\n",
    "    print(\"=\"*80)  \n",
    "    print('\\nAccuracy of naive: {:.2f}%'.format(accuracy_score(data[trait], y_naive)))\n",
    "    print(classification_report(data[trait], y_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems yes. Our model ~two times more precise than naive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
