{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Texts as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out short texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 189.076039088 \n",
      "Median: 61.0 \n",
      "Min: 1 \n",
      "Max: 16384\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "print('Mean:', lens.mean(),\n",
    "      '\\nMedian:', np.median(lens), '\\nMin:', min(lens), '\\nMax:', max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.32% shorter than 700\n",
      "16.88% longer than 200\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 700, 200\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4532, 10)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data = pd.merge(texts, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data.text = data.text.str.lower()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def build_model(data, X, y, vectorizer, model):\n",
    "    print(\"=\"*80)\n",
    "    print('BUILDING MODEL FOR {}'.format(y))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[X], data[y], test_size=0.1)\n",
    "    \n",
    "    print('Train sample: {}\\nTest sample: {}'.format(len(X_train), len(X_test)))\n",
    "        \n",
    "    train_vec = vectorizer.fit_transform(X_train)\n",
    "    test_vec = vectorizer.transform(X_test)\n",
    "     \n",
    "    print('\\nIncluded tokens ({})'.format(train_vec.shape[1]))\n",
    "    print(np.array(vectorizer.get_feature_names())[np.random.randint(0, len(vectorizer.get_feature_names()), 20)])\n",
    "    print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "    print(np.array(list(vectorizer.stop_words_))[np.random.randint(0, len(vectorizer.stop_words_), 20)])\n",
    "    \n",
    "    model.fit(train_vec, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(train_vec)\n",
    "    print('\\nMAPE on training sample: {:.2f}%'.format(mape(y_train, y_train_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_train, y_train_pred)))\n",
    "\n",
    "    y_test_pred = model.predict(test_vec)\n",
    "    print('\\nMAPE on test sample: {:.2f}%'.format(mape(y_test, y_test_pred)))\n",
    "    print('R2 on training sample: {:.3f}'.format(r2_score(y_test, y_test_pred)))\n",
    "    \n",
    "    print('\\nHigh pole')\n",
    "    #[print(a) for a in sorted(list(zip(model.coef_, vectorizer.get_feature_names())), reverse=True)[0:5]]\n",
    "    print('\\nLow pole')\n",
    "    #[print(a) for a in sorted(list(zip(model.coef_, vectorizer.get_feature_names())))[0:5]]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1123)\n",
      "['–¥—Ä—É–≥–∏—Ö' ', –∫–æ—Ç–æ—Ä—ã–π' '–µ–π' '—Ç–æ–º—É' ', –∏–∑' '—Ç–æ–º ,' '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ ,'\n",
      " '# —Ü–∏—Ç–∞—Ç—ã_–∏–∑_–ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ' '–∫—Å—Ç–∞—Ç–∏' '‚Äî' '–≤–æ–ø—Ä–æ—Å—ã' ', –∏–∑' '=' '–∏–º' '‚Äî —ç—Ç–æ'\n",
      " '—ç—Ç–æ–º' '–º–∏—Ä–∞' ', —ç—Ç–æ' '! ! !' '—Å–≤–æ—é']\n",
      "\n",
      "Excluded tokens (438187)\n",
      "['–Ω–µ –¥–µ–Ω—É—Ç ,' 'games' '–±–∞–ª–∫–æ–Ω–µ' '–Ω–∏—Ö –∏' '—á–∏—Å—Ç–æ—Ç–æ–π' '–≤–∏–Ω–æ–π —Ç–≤–æ–∏–º –∫–æ–ª–µ–Ω—è–º'\n",
      " ', –∑–∞–¥–∞–≤–∞—Ç—å' '—Ü–µ–Ω–Ω—ã–π .' '–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞' '—Ç–æ–ª—å–∫–æ —è' '–≤ —Å–µ–±—è –ª–∏'\n",
      " '—Ç–∞–∫–∏–º —É—Ä–æ–¥—Ü–µ–º..' '–æ—â—É—â–µ–Ω–∏–π ?' '–∏ –ø—Ä–∏–¥—É–º–∞–ª–∏ –æ–¥–Ω–æ' '—á—Ç–æ —Å–ª–æ–≤–∞—Ä—å'\n",
      " '–¥–µ—Ç–∞–ª—è—Ö –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å' '–∑–∞—Å–ª—É—à–∞–ª–∞—Å—å' '–ø–æ–ø–∞–ª–∞ –≤ –ø—Ä–µ–¥–ø–∏–∫–æ–≤—É—é'\n",
      " ', –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä–æ–π' '–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ—Å—Ç—å']\n",
      "\n",
      "MAPE on training sample: 6.54%\n",
      "R2 on training sample: 0.872\n",
      "\n",
      "MAPE on test sample: 15.83%\n",
      "R2 on training sample: 0.268\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX2_A\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1128)\n",
      "['–º–æ–µ–≥–æ' '–∏–±–æ' '—Å–∏–ª—å–Ω–æ' '–¥–≤—É—Ö' '–Ω–æ –∏' '—Å–µ–Ω—Ç—è–±—Ä—è' '—Å–ª–æ–≤–Ω–æ' '–∫–æ—Ç–æ—Ä—ã–º'\n",
      " '–≤—Ä–µ–º–µ–Ω–∏' '–º–∏–Ω—É—Ç' '–º–∏–Ω—É—Ç' '–¥–µ–ª–µ' '–æ–¥–Ω—É' '. )' '–∑–Ω–∞—é' '—Ç–æ—á–Ω–æ' '—Ç–∞–º'\n",
      " '—á–µ–ª–æ–≤–µ–∫–∞' '–≤—Å–µ ,' '–ø–æ—Å–ª–µ–¥–Ω–∏–π']\n",
      "\n",
      "Excluded tokens (439051)\n",
      "['–¥–∏–∞–≥–Ω–æ–∑ . –ø–æ—Å–ª–µ–¥–Ω–∏–π' '! ! for' '—Å–ª—É—á–∞–µ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞' '—Ç–∞—Ä–∞–Ω—è –∏—Ö'\n",
      " '–ø—Ä–æ–π–¥–∏—Ç–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ' '. –æ–¥–Ω–∞–∂–¥—ã –æ—Ä–ª–æ–≤' '—É—á–∏—Ç–µ–ª—å –ø–æ—Å–ª–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è'\n",
      " '–¥–æ–≤–µ—Ä–∏–µ .' '. –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏' '–Ω–µ–∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π—Å—è' ', –≤–µ–¥—É—â–∞—è'\n",
      " '–∑–∞ –µ–¥–∏–Ω—É—é —Ä–æ—Å—Å–∏—é' '–æ—á–µ–Ω—å –≤–∞—à–µ–≥–æ' '–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è‚Ä¶' '–≤—Å–µ–º–∏—Ä–Ω–æ–µ –Ω–∞—Å–ª–µ–¥–∏–µ ¬ª'\n",
      " '–∏ —Ä–µ—Å—É—Ä—Å–∞–º —Å–æ—Å—Ç–æ—è–ª—Å—è' '–≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞' ', –ª—é–±–ª—é —Ç–∞–∫' '–∞ —Å—Ç–µ–Ω—É'\n",
      " '—Ä–∞–∑–ª—É–∫ . –∑–¥–µ—Å—å']\n",
      "\n",
      "MAPE on training sample: 6.06%\n",
      "R2 on training sample: 0.873\n",
      "\n",
      "MAPE on test sample: 15.34%\n",
      "R2 on training sample: 0.328\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX3_C\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1120)\n",
      "['... –∏' '–ª—é–±–æ–≤—å' '–¥–Ω–µ–π' '–±—ã—Å—Ç—Ä–æ' '—Ö–æ—Ç—è –±—ã' '! –∏' ', –≤' '–∏ —Ç–∞–∫' '–ø—É—Å—Ç—å'\n",
      " '—á—Ç–æ –≤—ã' '—á—Ç–æ –Ω–µ' '—á—Ç–æ–±' '—á—Ç–æ–±' '—á—Ç–æ –≤' '–ø–æ–Ω–∏–º–∞—é' '–æ—Ç–≤–µ—Ç' '–∂' 'c )'\n",
      " '–µ—â–µ –∏' '—Ç–æ–º']\n",
      "\n",
      "Excluded tokens (437544)\n",
      "['–ø—É—à–∫–∏–Ω—Å–∫–∏–π !' '–ª–∏—Ç—Ä—ã' '—á—Ç–æ –º–µ–¥–∏–∫–∏ –Ω–µ–æ–±—ã—á–∞–π–Ω–æ' '–µ—Å–ª–∏ –Ω–∞ –∑–µ–º–ª–µ'\n",
      " '–Ω–∞—Å–µ–ª–µ–Ω–∏–∏ –≤ 5' '–Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ—Ä–∞' '–ª–µ–¥–∏ (' '–Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–Ω–∏–≥ ...'\n",
      " '–±–∞—à–º–∞–∫–æ–≤ ,' '–∑–Ω–∞—Ç—å , —á—Ç–æ' '—Ç–∞–∫–∞—è —Ü–µ–ø–æ—á–∫–∞' '—Å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤–∂–∏–≤–ª–µ–Ω–∏–µ–º'\n",
      " '–≤ –ª—é–±–æ–π –æ—Ç–¥–∞–ª–µ–Ω–Ω–æ—Å—Ç–∏' '–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –∏ ,' '–≤–∑–ª–æ–º–∞—Ç—å –º–æ—é' '—É—Ä–æ–≤–µ–Ω—å —Å–µ—Ä–≤–∏—Å–∞'\n",
      " '—Ç–µ–±–µ —É—Ç—Ä–æ–º ,' '–∫–æ—Ñ–µ–π–Ω–æ–≥–æ —Å–µ—Ä–≤–∏–∑–∞ –∏' '–≤ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å' '—Ç–∏–ø–∞ ?']\n",
      "\n",
      "MAPE on training sample: 7.47%\n",
      "R2 on training sample: 0.851\n",
      "\n",
      "MAPE on test sample: 17.94%\n",
      "R2 on training sample: 0.202\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX4_E\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1133)\n",
      "['–∏—Å—Ç–æ—Ä–∏–∏' ', –∫–æ—Ç–æ—Ä—ã–µ' '–Ω—É' '—á–µ–º—É' '—á–µ–º—É' '–Ω–∞ –≤—Å–µ' '–∫–Ω–∏–≥–∏' '! ¬ª' '–≥–æ–≤–æ—Ä–∏—Ç—å'\n",
      " '–Ω–∞–π—Ç–∏' '–ø–∏—Å–∞—Ç—å' '–∏ —è' 'my' '–±—ã' '–ø–æ–¥' '–º–Ω–æ–≥–∏–µ' '–∑–Ω–∞–µ—Ç–µ' '! ``' '—Å—Ä–µ–¥–∏'\n",
      " '—Ç–µ–º']\n",
      "\n",
      "Excluded tokens (439078)\n",
      "['? –ø–æ—á–µ–º—É —Ç—ã' '–¥–ª—è —Ç–µ–±—è —Å–ª–∏—à–∫–æ–º' '—Ä–µ–∞–ª—å–Ω—ã–π —à–∞–Ω—Å –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å'\n",
      " '–∂–µ–Ω—â–∏–Ω –ø—Ä–∏ –º—ã—Å–ª–∏' '–∏–¥—É—Ç , –∞' '–∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –ø–æ–¥–ª–∏–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–∞'\n",
      " '–∏ –º–æ–Ω–æ–≥—Ä–∞—Ñ–∏–π' 'forget the sunshine' 'fool in your'\n",
      " '–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–¥–µ–∞–ª–æ–º' 'us for' '—á–µ—Ä–Ω–æ–±—ã–ª—å –∏ —Ñ—É–∫—É—à–∏–º—É'\n",
      " ', –Ω–∏—á–µ–≥–æ –Ω–æ–≤–æ–≥–æ' ', more' '–ø–æ–¥–æ–±—Ä–∞—Ç—å —Å–ª–æ–≤–∞ –ª—É—á—à–µ' '–≤ –∞–ø—Ç–µ–∫–µ :'\n",
      " 'home is parochial' 'confess that your' 'or ,' '—ç—Ç–æ —á–µ—Å—Ç—å']\n",
      "\n",
      "MAPE on training sample: 6.07%\n",
      "R2 on training sample: 0.863\n",
      "\n",
      "MAPE on test sample: 17.05%\n",
      "R2 on training sample: 0.137\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX5_O\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1132)\n",
      "['–∏ –∫–∞–∫' '–¥—É—à–∏' '—á–µ–ª–æ–≤–µ–∫ ,' ', —á—Ç–æ –≤—ã' '–∂–∏–∑–Ω–∏' '—Ç–∞–∫–∏–µ' '–∏—Å–∫–∞—Ç—å'\n",
      " '–∫–∞–∂–µ—Ç—Å—è , —á—Ç–æ' '—É –Ω–µ–≥–æ' '—Ö–æ—á–µ—Ç—Å—è' '... –∏' '–ø–∞—Ä—É' '–∫–∞–∫ –±—ã' '? )' '–Ω–∞—à'\n",
      " '–∑–∞—Ç–æ' '–∏ —Ç—ã' '—Å–æ–±–æ–π' '–∫—Ç–æ-–Ω–∏–±—É–¥—å' '. —á—Ç–æ']\n",
      "\n",
      "Excluded tokens (438735)\n",
      "['–∞ –º–∞–ª—å—á–∏–∫ —Ä—è–¥–æ–º' '–Ω–¥–∞ .' '—Ç–≤–æ–∏–º –¥—ã—Ö–∞–Ω–∏–µ–º' '–∂–µ —Ä–µ–±—è—Ç–∞ –∑–¥–æ—Ä–æ–≤–æ' '–≥–∞–∑–µ–ª—å–∏–º–∏'\n",
      " '–¥–æ–ª–∂–Ω—ã –ª–∏' '–≥–µ–Ω–∏–∞–ª—å–Ω–æ–≥–æ –æ—Ç' ': [ id2896530|–∫–∞—Ç—Ä–∏–Ω—ä' '–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å . –µ—Å—Ç—å'\n",
      " 'silence is' '—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏' '–æ–¥–Ω–æ–≥–æ –æ—Ç—Ü–∞' '–ª–∏—Ü–æ –ø–æ–ª—É–Ω–æ—á–Ω–æ–µ ,'\n",
      " ', –∫–∞–∫ –º–µ—Å—è—Ü–∞' '–∏ –¥–æ–±–∞–≤–∏–ª–∞ ,' ', –∫—Ç–æ –ø–æ–¥–ø–∏—Å–∞–ª—Å—è' '–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º'\n",
      " '–ø–æ–ª–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å' '—Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –ø—Ä–µ–¥–º–µ—Ç' ', –ø–∞–±–ª–∏–∫']\n",
      "\n",
      "MAPE on training sample: 6.97%\n",
      "R2 on training sample: 0.825\n",
      "\n",
      "MAPE on test sample: 16.92%\n",
      "R2 on training sample: 0.100\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX6_H\n",
      "================================================================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (1132)\n",
      "['—Ñ–æ—Ç–æ' '–≤—Å–µ–π' '–ø—Ä–∏' '. —Å–µ–≥–æ–¥–Ω—è' ', –∏ —è' '–¥—Ä—É–≥–æ–º' '—Å–ª—É—á–∞–µ' '—Ç–µ—Ö , –∫—Ç–æ'\n",
      " '–∑–∞–≤—Ç—Ä–∞' '–≤—Å—è' '—ç—Ç–æ–π' '—á—Ç–æ –µ—Å–ª–∏' '—Ç–æ—Ç' '—Å–¥–µ–ª–∞—Ç—å' '? )' ', –∏ –≤' '—Å–º–µ—Ä—Ç–∏'\n",
      " '–±–æ–ª—å—à–µ –Ω–µ' '–ø–æ—Å–ª–µ–¥–Ω–∏–µ' '–¥–µ–Ω–µ–≥']\n",
      "\n",
      "Excluded tokens (438931)\n",
      "['—É–ª–∏—Ü–µ —Ç–∞–∫–∞—è –ø–ª–æ—Ö–∞—è' '–±—ã–ª–æ –ø–ª–µ–≤–∞—Ç—å' '–∏–≥—Ä–∞–π—Ç–µ' ': —É–ª' '–ø–æ–∑–∂–µ ,'\n",
      " ', –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è' ') . –≤–æ–∑–≤—Ä–∞—â–∞—é—Å—å' '–≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞—É—á—Ä—É–∫–∞' '–±–µ–Ω—É'\n",
      " '—é–∂–Ω–æ—É—Ä–∞–ª—å—Å–∫–∏–µ' '—Å–Ω–µ–∂–Ω—ã–º –≥–æ—Ä–æ–¥–æ–º' '—Ç—è–Ω–µ—Ç .' '–∫—Ä–µ–ø–∫–æ —Å–ø–∏–º' 'harris'\n",
      " '–≤–∞—Å –µ—Å—Ç—å –±—Ä–∞—Ç—å—è' '–∏ —Ö–æ—á–µ—Ç—Å—è –ø–æ–¥–æ–π—Ç–∏' '—É–º–Ω–µ–π' '–±–∏–æ–ª–æ–≥–∏—é –Ω–∞'\n",
      " '–∂–∏–∑–Ω–µ—É—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ (' '–≤—Ä–∞–≥–∏ –Ω–µ..']\n",
      "\n",
      "MAPE on training sample: 6.75%\n",
      "R2 on training sample: 0.858\n",
      "\n",
      "MAPE on test sample: 16.07%\n",
      "R2 on training sample: 0.302\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    lm = RandomForestRegressor()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 30, \n",
    "                         max_df = 0.7, \n",
    "                         max_features = 10000)\n",
    "    build_model(data, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - concatenated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 2)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts['text'] = texts['text'].apply(str).apply(str.lower)\n",
    "texts_conc = texts.groupby('owner_id')['text'].apply(lambda x: ' <ps> '.join(x))\n",
    "texts_conc = pd.DataFrame(texts_conc.reset_index())\n",
    "texts_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 70617.0566038 \n",
      "Median: 26465.5 \n",
      "Min: 85 \n",
      "Max: 918554\n"
     ]
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts_conc.text])\n",
    "print('Mean:', lens.mean(),\n",
    "      '\\nMedian:', np.median(lens), '\\nMin:', min(lens), '\\nMax:', max(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% shorter than 10000000000\n",
      "95.28% longer than 1000\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 10**10, 1000\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts_conc.text])\n",
    "texts_conc = texts_conc[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 10)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data2 = pd.merge(texts_conc, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING MODEL FOR HEX1_eX\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['–Ω–µ–≥–æ' '–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ' '—Ä–µ—à–∏–ª' 'that' '—Ü–µ–ª—ã–π' '—Ö–æ—Ä–æ—à–∏–π' '—Ñ–æ—Ç–æ' '–∫—Ä–æ–º–µ'\n",
      " '–º–Ω–µ–Ω–∏–µ' '–±–ª–∞–≥–æ–¥–∞—Ä—è' '–º–æ–∑–≥' '–¥–æ–º' '—Ö–æ—Ç—è' '—Å–Ω–∞—á–∞–ª–∞' '–ø–æ—á—Ç–∏' '—Å—Ç–æ—Ä–æ–Ω—É'\n",
      " '–ø–æ—Å—Ç–æ—è–Ω–Ω–æ' '—Ü–≤–µ—Ç—ã' '–º–æ–≥–ª–∏' '–≥–æ–≤–æ—Ä—è—Ç']\n",
      "\n",
      "Excluded tokens (133131)\n",
      "['–±–∏—Ç—å' '–Ω–∏—á—Ç–æ–∂–Ω–æ–≥–æ' '–∑–Ω–∞–∫–æ–º–æ-—Ç–æ' '—Å–∞—à–µ' '—Ä–∞—Å—Ç–∞–º–∞–Ω' '–≤–ø–µ—á–∞—Ç–ª—è–µ—Ç' '—Å–æ—Å–µ–¥–Ω—è—è'\n",
      " '–∞—Ä–≥–µ–Ω—Ç–∏–Ω—É' '—Å–∞—à—É–ª–µ–π' '—Å–∫—Ä—ã—Ç–∞—è' '–∑–Ω–∞–∫–æ–º—ã–º–∏' '–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏' '–ø—Ä–æ—Å—ã–ø–∞–µ—Ç—Å—è'\n",
      " '—É–Ω–∏–≤–µ—Ä—Å–∏—Å—Ç–µ—Å–∫–æ–≥–æ' '–Ω–∞–ª–∏–ª' '–≥–æ—Ä–∞—Ü–∏–π' '—Å—Ä–∞–≤–Ω–∏–≤–∞–ª–∞' 'dumb' '–∫–ª–∞–Ω' '—Å—Ä–µ–¥–Ω—é—é']\n",
      "\n",
      "MAPE on training sample: 6.48%\n",
      "R2 on training sample: 0.833\n",
      "\n",
      "MAPE on test sample: 16.32%\n",
      "R2 on training sample: 0.324\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX2_A\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['so' '–≤–∫–æ–Ω—Ç–∞–∫—Ç–µ' '—Ö–æ—Ä–æ—à–µ–≥–æ' 'like' '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π' '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã' '–Ω–∞–∫–æ–Ω–µ—Ü-—Ç–æ'\n",
      " '–≥—Ä—É–ø–ø—ã' '***' '–≤—Å–µ–º–∏' '@' '–∫–æ–≥–æ-—Ç–æ' '–æ–±' '–∫–∞–∂–µ—Ç—Å—è' '–¥–µ–ª–∞—é—Ç' 'xd'\n",
      " '–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ' '–¥–Ω–µ–º' '–Ω–∞—à–µ' '–≤–µ—Ç–µ—Ä']\n",
      "\n",
      "Excluded tokens (109938)\n",
      "['–ø–æ–≥–∏–±–∞–µ—Ç' '–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π' '—è—Ä–æ—Å–ª–∞–≤' '–∑–∞–º–µ–ª–æ' '–±–ª–æ–≥–µ—Ä–∞–º' '–æ–ª–∏–º–ø–∏–∞–¥—É'\n",
      " '–∑–≤–µ—Ä–µ–π' '–ª—É—Ü–∏–π' '–ø–æ–≥–æ–≤–æ—Ä–∏' 'dreams' '–ø–æ–Ω—Ç–∞' '—Å—Ç—É–∫-—Å—Ç—É–∫' '—Å—É–º–µ—Ä–∫–∏'\n",
      " 'freebsd' '–≤–Ω–µ—à–Ω–æ—Å—Ç—å—é' '–∫–æ–º–µ—Ç–∞' 'kalos' '–ø–æ–∏—â–∏—Ç–µ' '—Å–∏–ª–ª–∞–±—É—Å' '—Ç—É–¥–µ–Ω—Ç–æ–≤']\n",
      "\n",
      "MAPE on training sample: 6.43%\n",
      "R2 on training sample: 0.838\n",
      "\n",
      "MAPE on test sample: 15.64%\n",
      "R2 on training sample: 0.244\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX3_C\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['–≥–ª–∞–∑–∞—Ö' '–∏—â—É' 'was' '–∫–æ–Ω—Ü–∞' '–¥—Ä—É–≥–æ–π' '–∫–∞–∂–¥–æ–≥–æ' 'd0' '–Ω–∞–æ–±–æ—Ä–æ—Ç' '—Å–∞–º—ã–µ'\n",
      " '—Å–≤–æ–∏–º' '–ø—Ä–æ—à—É' '–∂–∏–≤—É—Ç' '—Å–µ—Ä–¥—Ü–∞' '–º–µ—Å—Ç–µ' '–≥–æ–≤–æ—Ä—è' '–º–∞–º–∞' '—Ä–∞–¥–æ—Å—Ç—å' '–Ω–æ—á–∏'\n",
      " '6.' 'be']\n",
      "\n",
      "Excluded tokens (131813)\n",
      "['–Ω–æ–±–µ–ª–µ–≤–∫–∞' '–æ–±—Å—É–∂–¥–µ–Ω–∏–∏' '—Ç–∏–∫—Å–∏' '18-00' '—Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ' '–ø–ª–∞—Ç–Ω–æ.'\n",
      " '–ø—Ä–æ–º—ã–≤–∞–Ω–∏—è' '–≤–µ—á–Ω–∞' '–ø—Ä–∏–≤—è–∑–∞–Ω–æ' '–∑–∞–∫–∏–≤–∞–ª' '–±–µ–∑–º–æ–ª–≤–Ω–æ–µ' '–ø–µ—Ä–µ–±–∏–≤–∞—é—Ç—Å—è'\n",
      " '–Ω–µ–æ–±—ã—á–∞–π–Ω–æ–π' '–Ω–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–µ–π' 'touchent'\n",
      " '//bulalex.ru/2011-03-21-16-31-18/roomesc' '–ø—Äa–≤–¥—ã' '–ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É—é'\n",
      " '–ø—Ä–µ—Å–ª–∏' '–≥–ª–∞–∑–Ω–æ–º']\n",
      "\n",
      "MAPE on training sample: 6.73%\n",
      "R2 on training sample: 0.844\n",
      "\n",
      "MAPE on test sample: 28.22%\n",
      "R2 on training sample: 0.079\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX4_E\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['—Ä–∞–¥–æ—Å—Ç–∏' '—Ö–≤–∞—Ç–∏—Ç' '1.' '–ª—é–±–æ–π' 'just' '–º–µ—Å—è—Ü–∞' '–Ω–æ–≤—ã–µ' '–æ–¥–Ω—É' '–Ω–æ—á—å—é'\n",
      " '—Ä–µ—à–∏–ª' '–º–µ—Å—Ç–µ' '–ø–æ—á—Ç–∏' '—á–∞—Å—Ç—å' '–Ω–∞–∫–æ–Ω–µ—Ü-—Ç–æ' '–¥—Ä—É–≥–∏—Ö' '–∫–æ' '—Ä–æ—Å—Å–∏—è'\n",
      " '–¥–æ–ª–≥–æ' '–Ω–µ–¥–µ–ª–∏' '–≤—Å—Ç—Ä–µ—á–∏']\n",
      "\n",
      "Excluded tokens (125321)\n",
      "['–∫—ñ–Ω—Ü—è' '—Ä–∞–¥—É–∂–Ω—ã—Ö' '*–ø–ª–∞—á—É*' '–≥–ª–∞–≤–Ω–µ–π—à–∏–º' 'halen' 'üòéüíµüí∏üí°' '–º–∞—Ä—å–∏–≤–∞–Ω–Ω–∞'\n",
      " '–ø–ª–æ–¥–æ—Ç–≤–æ—Ä–Ω–æ' '–≤—Å—è—á–µ—Å–∫–∏–µ' '—Ü–µ–Ω–∏—Ç—å' '–ø–æ—è–∞–∏–ª–æ—Å—å' '–¥–æ–∫—É–º–µ–Ω—Ç–∞–ª–∫–∞' '–º–æ—Ä–∞–ª—å–Ω—ã'\n",
      " '–æ–±–∏—Ç–∞—é—Ç' '–≤–æ–µ–Ω–Ω—ã–º' '–≤—ã–Ω—É–ª–∏' '—Ç—Ä–æ—è–∫–∞' '—à–∞—Ç–∫–∏–º' '|—ç–ª–∏–µ–∑–µ—Ä'\n",
      " '—Å–æ–Ω–µ—á–∫–æ–π-–º–∞–∫–∞—Ä–æ–Ω–µ—á–∫–æ–π']\n",
      "\n",
      "MAPE on training sample: 7.47%\n",
      "R2 on training sample: 0.768\n",
      "\n",
      "MAPE on test sample: 26.54%\n",
      "R2 on training sample: -0.105\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX5_O\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['–º–∞–ª–µ–Ω—å–∫–∏–π' 'd0' '–æ–±—ã—á–Ω–æ' '–¥–∞–ª–µ–∫–æ' '–∞—Ö' 'd1' '—Ä–∞–±–æ—Ç—É' '–¥–≤–µ—Ä—å' '–º–µ—Å—Ç–µ' '–∫–æ'\n",
      " '–ø–æ—Å–ª–µ–¥–Ω–∏–µ' '–æ—Ç–≤–µ—Ç' 'know' '–ø—Ä—è–º' '—è–∑—ã–∫–∞' '–Ω—É–∂–µ–Ω' '–¥—Ä—É–≥–æ–≥–æ' '—Å–∞–º–æ–≥–æ'\n",
      " '–æ–±—â–µ–º' '4.']\n",
      "\n",
      "Excluded tokens (117301)\n",
      "['–∫–∞—Ä–ª–æ–º' '–º—É–∑–µ–π–Ω—ã–π' '–∫–æ—Ä–∏–ª–∞' '–ª–µ–∫–∞—Ä—è' '–≤—ã–ª–∞' '—à—Ç–∞–Ω–∏—à–∫–∞—Ö' '—Ä–∞–¥—É–µ–º—Å–æ'\n",
      " '–∫–∞–Ω–∞—Ç–∏–∫–∏' '–æ–∫–∞–∑–∞–Ω–∏—è' '–≥—É—â–∏' '–ø–æ–º—è—Ç–∞' '—Å–ø–µ–∫—Ç–æ—Ä' '–æ—Å—Ç–∞–ª—å–Ω–æ' '–∫–ª—é—à–∫–æ–π'\n",
      " '–∑–∞–∑—ã–≤–∞—é' '—Ä–∞–±–æ—á–∏–º' '–∏–∑–≤–∏–ª–∏–Ω–∞' '–ø–æ–¥–æ—Å–ø–µ–ª–æ' '–∑–∞–±–µ—Ä—ë—Ç' 'id234517589|—Ç–∏–º']\n",
      "\n",
      "MAPE on training sample: 7.34%\n",
      "R2 on training sample: 0.790\n",
      "\n",
      "MAPE on test sample: 15.01%\n",
      "R2 on training sample: -0.326\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "================================================================================\n",
      "BUILDING MODEL FOR HEX6_H\n",
      "================================================================================\n",
      "Train sample: 92\n",
      "Test sample: 11\n",
      "\n",
      "Included tokens (1000)\n",
      "['—Å–∞–º–∏' '—á–∞–π' '50' '–∫–µ–º' '1.' '–ø–æ–ª–Ω–æ—Å—Ç—å—é' '–∫–≥' '–ø–ª–æ—Ö–æ' '–±–æ–ª—å—à–æ–π' '–ª—é–±–∏—Ç—å'\n",
      " '–¥–æ–º–æ–π' '–æ–±–æ–∂–∞—é' '–¥–∞–ª–µ–∫–æ' '–≤–∏–¥–Ω–æ' '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ' '–≤—Å—è' '–∑–Ω–∞—Ç—å' '–≤—ã–≥–ª—è–¥–∏—Ç'\n",
      " '–≤–æ–¥—ã' '–≥–æ—Ç–æ–≤–∞']\n",
      "\n",
      "Excluded tokens (124511)\n",
      "['–µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã—Ö' '–∏—Å–∫—É—Å—Å—Ç–≤–æ–≤–µ–¥' '—Ç–æ–π—á–∞' '–ø—å–µ—Å—É' '—Ä–æ–ª–ª—Å-—Ä–æ–π—Å–∞' '–∏–∑–≤–µ—Å—Ç–Ω—ã'\n",
      " '—à–ª—é—Ö—É' '—É–¥–∏–≤–∏–ª–∏—Å—å' 'inspiring' '–±–∞—Ç–ª–µ—Ä' '–∫–æ–ø–∏–ª–∞' '–≥–ª—è–Ω—Ü–µ–≤–æ–º' '—Å–æ—Å–µ–¥—Å–∫–∞—è'\n",
      " '–ø—Ä–∏—Ö–≤–∞—Ç–∏–ª' '—Å—Ö–ª–æ–ø—ã–≤–∞–µ—à—å' '–ª–∏—á–µ—ã–π' '–ø–æ-—Å—Ç–∞—Ä–æ–º—É' '–≥–ª—è–∂—É—Å—å' '–±–∞–ª–¥–µ–ª–∞'\n",
      " 'cerises']\n",
      "\n",
      "MAPE on training sample: 7.93%\n",
      "R2 on training sample: 0.806\n",
      "\n",
      "MAPE on test sample: 18.72%\n",
      "R2 on training sample: -0.098\n",
      "\n",
      "High pole\n",
      "\n",
      "Low pole\n",
      "\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    lm = RandomForestRegressor()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 10, \n",
    "                         max_df = 0.7, \n",
    "                         max_features = 1000)\n",
    "    build_model(data2, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Nominal traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 8)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load psychological data\n",
    "cols = ['id', 'sex', 'HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']\n",
    "traits = pd.read_csv('data/survey_data.csv', sep=';', decimal=',', usecols=cols)\n",
    "traits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_groups(x, dev=1, M=50, SD=10):\n",
    "    if x > M+dev*SD:\n",
    "        return 'high'\n",
    "    elif x < M-dev*SD:\n",
    "        return 'low'\n",
    "    else:\n",
    "        return 'average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEX1_eX\n",
      "high       53\n",
      "average    51\n",
      "low        48\n",
      "Name: HEX1_eX_nom, dtype: int64\n",
      "HEX2_A\n",
      "average    58\n",
      "high       51\n",
      "low        43\n",
      "Name: HEX2_A_nom, dtype: int64\n",
      "HEX3_C\n",
      "average    53\n",
      "high       52\n",
      "low        47\n",
      "Name: HEX3_C_nom, dtype: int64\n",
      "HEX4_E\n",
      "high       57\n",
      "average    48\n",
      "low        47\n",
      "Name: HEX4_E_nom, dtype: int64\n",
      "HEX5_O\n",
      "high       56\n",
      "average    50\n",
      "low        46\n",
      "Name: HEX5_O_nom, dtype: int64\n",
      "HEX6_H\n",
      "average    54\n",
      "high       50\n",
      "low        48\n",
      "Name: HEX6_H_nom, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for trait in ['HEX1_eX', 'HEX2_A', 'HEX3_C', 'HEX4_E', 'HEX5_O', 'HEX6_H']:\n",
    "    scale = trait + '_nom'\n",
    "    traits[scale] = traits[trait].apply(set_groups, dev=0.5)\n",
    "    print(trait)\n",
    "    print(traits[scale].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38375, 2)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text data from db\n",
    "conn = sqlite3.connect('ud.db')\n",
    "c = conn.cursor()\n",
    "query = 'SELECT DISTINCT owner_id, text FROM posts WHERE text IS NOT NULL AND text != \"\";'\n",
    "texts = pd.read_sql(query, conn)\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.32% shorter than 700\n",
      "16.88% longer than 200\n"
     ]
    }
   ],
   "source": [
    "trsh_up, trsh_lo = 700, 200\n",
    "print('{:.2f}% shorter than {}'.format(lens[lens<trsh_up].shape[0]/lens.shape[0]*100, trsh_up))\n",
    "print('{:.2f}% longer than {}'.format(lens[lens>trsh_lo].shape[0]/lens.shape[0]*100, trsh_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array([len(str(t)) for t in texts.text])\n",
    "texts = texts[(lens < trsh_up) & (lens > trsh_lo)]\n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(s):\n",
    "    rgxp = '[\\`\\)\\(\\|¬©~^<>/\\'\\\"\\¬´‚Ññ#$&\\*.,;=+?!\\‚Äî_@:\\]\\[%\\{\\}\\\\n]'\n",
    "    return re.sub(' +', ' ', re.sub(rgxp, ' ', s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     –º–∞—Å—Ç–µ—Ä —Ç—ã –≥–æ–≤–æ—Ä–∏–ª —á—Ç–æ –µ—Å–ª–∏ —è –ø–æ–∑–Ω–∞—é –∫—Ç–æ —è —Ç–æ ...\n",
       "1    –æ–¥–Ω–∞–∂–¥—ã —Å–æ–ª–¥–∞—Ç –æ—Ö—Ä–∞–Ω—è–≤—à–∏–π –¥–æ—Ä–æ–≥—É –æ—Å—Ç–∞–Ω–æ–≤–∏–ª –±—É–¥...\n",
       "2    –ª—é–¥–∏ —è —Ö–æ—á—É –∏–∑–≤–∏–Ω–∏—Ç—å—Å—è –ø–µ—Ä–µ–¥ –≤–∞–º–∏‚Ä¶–ø–µ—Ä–µ–¥ –≤—Å–µ–º–∏ ...\n",
       "3    what i plainly see before my eyes makes me fin...\n",
       "4    –∑–¥—Ä–∞–≤—Å—Ç–≤—É–π –º–∞–ª—å—á–∏–∫ –±–∞–Ω–∞–Ω–∞–Ω –µ—Å–ª–∏ —Ç—ã –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join data\n",
    "data3 = pd.merge(texts, traits, how='left', left_on='owner_id', right_on='id')\n",
    "data3.text = data3.text.str.lower().apply(cleanse)\n",
    "data3.shape\n",
    "data3.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(data, X, y, vectorizer, model):\n",
    "    print(\"=\"*40)\n",
    "    print('BUILDING MODEL FOR {}'.format(y))\n",
    "    print(\"=\"*40)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[X], data[y], test_size=0.1)\n",
    "    print('Train sample: {}\\nTest sample: {}'.format(len(X_train), len(X_test)))\n",
    "    train_vec = vectorizer.fit_transform(X_train)\n",
    "    test_vec = vectorizer.transform(X_test)\n",
    "    print('\\nIncluded tokens ({})'.format(train_vec.shape[1]))\n",
    "    print(np.array(vectorizer.get_feature_names())[np.random.randint(0, len(vectorizer.get_feature_names()), 20)])\n",
    "    print('\\nExcluded tokens ({})'.format(len(vectorizer.stop_words_)))\n",
    "    print(np.array(list(vectorizer.stop_words_))[np.random.randint(0, len(vectorizer.stop_words_), 20)])\n",
    "    model.fit(train_vec, y_train)\n",
    "    y_train_pred = model.predict(train_vec)\n",
    "    print('\\nAccuracy on training sample: {:.2f}%'.format(accuracy_score(y_train, y_train_pred)))\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    y_test_pred = model.predict(test_vec)\n",
    "    print('Accuracy on test sample: {:.2f}%'.format(accuracy_score(y_test, y_test_pred)))\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "BUILDING MODEL FOR HEX1_eX_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (832)\n",
      "['–≤–∞–∂–Ω–æ' '—Å–ª–∏—à–∫–æ–º' '–±—É–¥—Ç–æ' '—Ä–∞–∑' '–∫–æ–≥–æ' 'is' '–¥—É—à–µ' '–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è' '–ª–µ—Ç'\n",
      " '–Ω–µ –±—ã–ª–æ' '–¥–µ–ª–∞' '–ø–µ—Ä–µ–¥' '–¥–∞–∂–µ' '–Ω–æ –Ω–µ' '–≤–∑–≥–ª—è–¥' '–æ—á–µ–Ω—å' '—Ç–∞–∫ –∫–∞–∫' '—Å–µ–±–µ'\n",
      " '—Å—Ç–æ—Ä–æ–Ω—ã' '–≤ —Ç–æ–º']\n",
      "\n",
      "Excluded tokens (398142)\n",
      "['–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é' '–∫–æ–µ-–∫–∞–∫–∏–º–∏' '–Ω–∞—Å–ª–∞–¥–∏—Ç—å—Å—è —ç—Ç–∏–º –ø–æ–º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å'\n",
      " '–±—É–¥—É –ø–∏—Å–∞—Ç—å –æ' '–≤–æ–π–Ω–æ–π –∏' '–æ—Ç–ª–∏–≤–æ–≤' '—Ç–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è' '—Å—Ç—É–¥–µ–Ω—Ç–æ–º –∏ –Ω–µ'\n",
      " '—Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –ø–æ–ª–æ–∂–µ–Ω–∏–µ —Å–≤–æ–∏—Ö' '–Ω–∞–∫—É—Ä–µ–Ω–æ –æ—á–µ–Ω—å' '–∏–∑ –Ω–∏—Ö –≤–∏–¥–∏–º–æ'\n",
      " '–ø–æ—Ä—ã–≤–∏—Å—Ç–æ–≥–æ –º–∏—Ä–∞' '–º–æ–≥—É –±—ã—Ç—å —Ç–∞–º' 'p p s' 'id1775145 –∫–∏—Ä–∏–ª–ª–∞'\n",
      " '–ø–∞—Ä–∞ —à—Ç—É—á–µ–∫ –ª–∏—à–Ω–∏—Ö' '—Ç–≤–∏—Ç—Ç–µ—Ä–µ –æ—á–µ–Ω—å' 'obshem minutka tragizma'\n",
      " '–∏–ª–∏ —á–∏—Ç–∞—Ç—å' '—Ç—Ä–∞—Ç—å—Ç–µ –ø–ª—é—Å—ã —É']\n",
      "\n",
      "Accuracy on training sample: 0.64%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.61      0.71      0.66      1536\n",
      "       high       0.66      0.76      0.70      1516\n",
      "        low       0.66      0.35      0.46      1026\n",
      "\n",
      "avg / total       0.64      0.64      0.63      4078\n",
      "\n",
      "Accuracy on test sample: 0.55%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.49      0.60      0.54       163\n",
      "       high       0.62      0.68      0.65       188\n",
      "        low       0.46      0.22      0.30       103\n",
      "\n",
      "avg / total       0.54      0.55      0.53       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX2_A_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (850)\n",
      "['—Ö–æ—á–µ—à—å' '–Ω–∞—Å—Ç–æ–ª—å–∫–æ' '–ª–µ—Ç–æ' '–Ω–µ–≥–æ' '–¥–æ–º–æ–π' '–º–µ—á—Ç—ã' '–¥–∞–ª–µ–µ' '—Å–æ–≤—Å–µ–º' '–º–∏—Ä'\n",
      " '–ø–æ—Å–ª–µ' '–æ—â—É—â–µ–Ω–∏–µ' '–Ω–æ–≤–æ–≥–æ' '—Å—Ç–∞—Ç—å' '–≤–¥—Ä—É–≥' '–Ω–∏–∫–æ–≥–¥–∞ –Ω–µ' '–ª–∏—á–Ω–æ' '–∫–∞–∂–¥–æ–≥–æ'\n",
      " 'com' '–Ω–æ –≤' '–Ω–µ –Ω–∞–¥–æ']\n",
      "\n",
      "Excluded tokens (399254)\n",
      "['–∏ –∫–æ–π' '–∫–∞—Ñ–µ–¥—Ä–∞' '—á–µ—Ç—ã—Ä–µ –≥–æ–¥–∞ —É–∂–µ' '–∫–∞–∫ —É—á–∞—Å—Ç–Ω–∏–∫'\n",
      " '—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–Ω–∏–º–∞–µ—à—å —á—Ç–æ' '–∑–∞—Ä–µ–≤–æ –∫—Ä–∞—Å–Ω–∞ –≤–¥–æ–≤–æ—é' '–æ–±—â–µ–º –º–Ω–µ –æ–Ω'\n",
      " '—É—Ç—ë–∫—à–∏–º –≤' '–≤ —Ç–æ—Å–∫–µ' '–∞–ª—å–±–æ–º—ã –≤' '–¥—Ä—É–≥–∞ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–π—Ç–µ –≤–∞—à–∏'\n",
      " '–∂–∏–∑–Ω—å –±—ã–ª–∞ –±–æ–ª—å—à–µ' '–æ—á–µ–Ω—å –≤–∞–∂–Ω—É—é' '–µ –º–µ–∫—Å–∏–∫–∞' '–∏ —ç—Ç—É —á–µ—Ä–Ω—É—é'\n",
      " '—Ü–µ–Ω—Ç—Ä–µ —Ç–∞–∫–∏—Ö –≤–æ—Ç' 'everypony' '–ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏ —á–µ–º —Ç–∞–º'\n",
      " '—Ä–∞–∑–º–µ—Ä–µ–Ω–Ω–∞—è –∏ –ø–æ-—Å–≤–æ–µ–º—É' '–ø–∏–¥–∞—Ä–∞—Å—ã –ø–∏–¥–∞—Ä–∞—Å—ã –ø–∏–¥–∞—Ä–∞—Å—ã']\n",
      "\n",
      "Accuracy on training sample: 0.70%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.67      0.75      0.71      1615\n",
      "       high       0.75      0.72      0.73      1138\n",
      "        low       0.70      0.63      0.66      1325\n",
      "\n",
      "avg / total       0.70      0.70      0.70      4078\n",
      "\n",
      "Accuracy on test sample: 0.53%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.47      0.62      0.54       166\n",
      "       high       0.62      0.50      0.56       135\n",
      "        low       0.54      0.45      0.49       153\n",
      "\n",
      "avg / total       0.54      0.53      0.53       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX3_C_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (850)\n",
      "['–Ω–∏–∫–æ–ª–∞–π' '–ø—Ä—è–º–æ' '–∫–æ–Ω—Ü–∞' '–º–æ–∂–µ—Ç–µ' '–ø–æ—Ç–æ–º—É —á—Ç–æ' '–¥–≤–∞' '—Å–∏–ª—å–Ω–æ' '–∏–∑-–∑–∞'\n",
      " '–º–æ—é' '—Å–∞–º–∞—è' '–≤–∞—Å' '–ø–æ—Å—Ç–æ—è–Ω–Ω–æ' '–±–æ–ª—å—à–æ–µ' '–Ω–æ—á—å—é' '–Ω—Ä–∞–≤–∏—Ç—Å—è' 'in' '–µ—â–µ –∏'\n",
      " '–±—ã—Å—Ç—Ä–æ' '—è –Ω–µ' '–Ω–µ –º–æ–≥—É']\n",
      "\n",
      "Excluded tokens (398446)\n",
      "['they can change' 'house for the' '–Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ' '–æ–Ω –ø–ª–∞–≤–∞–µ—Ç'\n",
      " '—Ü–≤–µ—Ç—ã —Ñ–æ—Ç–æ—Å–µ—Å—Å–∏—è' '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ –≤—ã—Å—Ç—É–ø–∞–µ—Ç' '–º–æ—Å–∫–≤–µ-—Ç–æ –ø–æ–ª–æ–≤–∏–Ω–∞'\n",
      " '—ç—Ç–æ —á—É–¥–æ –µ—Å–ª–∏' '–ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ —É–±–µ–¥–∏—Ç—Å—è' '—è —Å–æ–ø–µ—Ä–µ–∂–∏–≤–∞—é —Å–∫–æ–ª—å–∫–æ' '–∂–∞–ª–∫–æ üòá'\n",
      " '–∂–∏–≤—É—Ç –¥–æ–ª–≥–æ' '–∏ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ' '–ø–æ–¥ –¥—É–¥–æ—á–∫—É' '–∂–∏–ª–∏ –¥–∏–≤–Ω—ã–µ'\n",
      " '–∫–∞–º—á–∞—Ç–∫–∞ –ø—Ä–µ–∫—Ä–∞—Å–Ω–∞ –∏' 'xxx –æ–¥–Ω–∞–∂–¥—ã —è' '—Å–∏–≥–∞—Ä–µ—Ç –Ω–æ –Ω–µ' '—á–µ—Ä–µ–∑ –≤—Å–µ —Å—Ç—Ä–∞—Ö–∏'\n",
      " '–æ—á–µ–Ω—å –Ω–µ–ø—Ä–æ—Å—Ç—ã–º –±—ã–ª–æ']\n",
      "\n",
      "Accuracy on training sample: 0.66%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.74      0.38      0.50      1044\n",
      "       high       0.65      0.79      0.71      1599\n",
      "        low       0.65      0.72      0.68      1435\n",
      "\n",
      "avg / total       0.67      0.66      0.65      4078\n",
      "\n",
      "Accuracy on test sample: 0.53%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.35      0.15      0.21       111\n",
      "       high       0.54      0.68      0.60       161\n",
      "        low       0.56      0.62      0.59       182\n",
      "\n",
      "avg / total       0.50      0.53      0.50       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX4_E_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (860)\n",
      "['–∑–Ω–∞—é' '–æ–±—â–µ–º' '—á—É—Ç—å' '–º–æ–∏–º' '–∂' '–∏ —Ç' '4' '–Ω—É–∂–µ–Ω' 'https' '–µ—Å–ª–∏' '–º–∏—Ä'\n",
      " '–ø–∏—Å–∞—Ç—å' '–º—ã—Å–ª–∏' '2' '–µ–π' '—Ö–æ—Ä–æ—à–æ' '–∏ –Ω–µ' '–ø–ª–æ—Ö–æ' '–Ω–∏–∫—Ç–æ' '–∞ –≤']\n",
      "\n",
      "Excluded tokens (399965)\n",
      "['—Å—Ç–∫' '—ç—Ç–∏–º –Ω–∏—á–µ–≥–æ' '–∞—Ñ–∏—à–∫–µ —Å–µ—Ä–∏–∞–ª–∞' '–ø–æ–ª–µ–π —Å 8' '—Ö–æ–ª–æ–¥–Ω—ã–º –ª–µ—Ç–Ω–∏–º –¥–Ω–µ–º'\n",
      " '—Å–≤–æ–π —Å—á–µ—Ç –æ—á–µ–Ω—å' '–æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å–¥–µ–ª–∞—é—Ç —Ö–æ—Ä–æ—à–∞—è' '—ç—Ç–æ–º—É —Ñ–∏–ª—å–º—É –∏–Ω—ã–º–∏'\n",
      " '–æ–ø–µ—Ä–∞—Ü–∏—é –Ω–æ' '—Å–ª—É—à–∞—Ç—å –¥—ã—à–∞—Ç—å' '–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø'\n",
      " '—Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —É—Ä–æ–≤–µ–Ω—å –Ω–∞—Ç—Ä–∏—è' '–∫–∞—Å—Å–µ —Å—Ç–∞—Ä–±–∞–∫—Å–∞' '–≤–µ–¥–µ—Ç—Å—è –Ω–∞ —ç—Ç–æ'\n",
      " '—Ö—Ä–∞–º zenkojidani' '–º—É–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–º–Ω—ã–µ –∏' '–∏ —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –ø–æ'\n",
      " '–æ–∑–≤—É—á–∏—Ç—å –µ–º—É' '—á—Ç–æ-—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é' '—Ç–∞–º —Ä–æ—Å']\n",
      "\n",
      "Accuracy on training sample: 0.68%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.72      0.61      0.66      1225\n",
      "       high       0.66      0.91      0.76      1990\n",
      "        low       0.79      0.26      0.39       863\n",
      "\n",
      "avg / total       0.70      0.68      0.65      4078\n",
      "\n",
      "Accuracy on test sample: 0.63%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.60      0.47      0.52       131\n",
      "       high       0.64      0.87      0.74       239\n",
      "        low       0.60      0.21      0.32        84\n",
      "\n",
      "avg / total       0.62      0.63      0.60       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX5_O_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (844)\n",
      "['—ç—Ç–∏' '—ç—Ç–æ–π' '–≥–¥–µ' '—Ä–µ–ø–æ—Å—Ç' '–Ω–æ–≤—ã–π' '—è –±—É–¥—É' '–≤ —Ä–æ—Å—Å–∏–∏' '–º—ã—Å–ª—å' '–º–µ—á—Ç—ã'\n",
      " '–∏–Ω–æ–≥–¥–∞' '–º—ã' '–º–µ—Ç—Ä–æ' '—Å–ª–æ–≤–Ω–æ' '–¥–∞–∂–µ –Ω–µ' '–µ—ë' '–∫–æ—Ç–æ—Ä–æ–º' '—Å—Ç–∞—Ç—å' '—Ä–∞–±–æ—Ç–∞'\n",
      " '–Ω–∞ –≤—Å–µ' '–±—ã—Å—Ç—Ä–æ']\n",
      "\n",
      "Excluded tokens (398377)\n",
      "['—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π' '—Ç–µ–ø–µ—Ä—å —É–∂–µ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å' '–ª–æ—Ç–∞' '–¥–∞ –Ω–∞–≤–µ—Ä–Ω–æ–µ'\n",
      " '–Ω–∞–¥–æ –¥–µ–ª–∞—Ç—å —Å–∫–∏–¥–∫—É' '04 03 vzryv-v-peterburgskom-metro-hronika' '—Ñ–æ—Ä–º–∞—Ç–∞'\n",
      " '–∂–∏–≤—É –≥–¥–µ-—Ç–æ –Ω–∞' '—á–µ–≥–æ –º–æ–∂–µ—Ç –¥–æ–≤–µ—Å—Ç–∏' '–Ω–µ –∑–∞–±—É–¥—å —Ä–µ–ø–æ—Å—Ç–Ω—É–ª'\n",
      " '–ø–∞–∫–µ—Ç–∞–º–∏ —á–µ–º—É –≤—ã' '–º–µ–Ω—è –≥–ª—è–¥–µ–ª–∏' '–∏ —Ç–∞–∫ –º–∏–Ω—É—Ç' '30 —á' '—Ö–æ—á–µ—Ç—Å—è –≤–∑—è—Ç—å –∏'\n",
      " '–Ω–∞—Ç—É—Ä—É' '–∂–µ—Ä–µ–±—Ü–µ' '–≤—ã—Ö–∏' '–º—è–≥–∫–æ–≥–æ —Ä–∞–∑–æ–º–ª–µ–≤—à–µ–≥–æ –Ω–∞—á–∏–Ω–∞—é'\n",
      " '–∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ http']\n",
      "\n",
      "Accuracy on training sample: 0.63%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.65      0.29      0.41      1013\n",
      "       high       0.61      0.89      0.72      1963\n",
      "        low       0.70      0.48      0.57      1102\n",
      "\n",
      "avg / total       0.64      0.63      0.60      4078\n",
      "\n",
      "Accuracy on test sample: 0.52%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.40      0.15      0.22       124\n",
      "       high       0.54      0.80      0.65       210\n",
      "        low       0.49      0.38      0.43       120\n",
      "\n",
      "avg / total       0.49      0.52      0.47       454\n",
      "\n",
      "\n",
      "========================================\n",
      "BUILDING MODEL FOR HEX6_H_nom\n",
      "========================================\n",
      "Train sample: 4078\n",
      "Test sample: 454\n",
      "\n",
      "Included tokens (841)\n",
      "['–¥—Ä—É–≥–æ–≥–æ' '–≤—Å–µ —á—Ç–æ' '—á–∏—Ç–∞—Ç—å' 'be' '–∫—Ç–æ –Ω–µ' '—á–µ–ª–æ–≤–µ–∫–∞' 'florist' '–Ω–∞—á–∞–ª–∞'\n",
      " '—ç—Ç–æ –±—ã–ª–æ' '—Ç–µ–±–µ' '–ø–µ—Ä–µ–¥' '–æ–±—Ä–∞–∑–æ–º' '—Å–æ–±–æ–π' '—Ç –¥' 'in' '- –∞' '–≤–∏–¥–µ–ª'\n",
      " '–Ω–∏–º–∏' '—á–µ–ª–æ–≤–µ–∫—É' '–∏–∑']\n",
      "\n",
      "Excluded tokens (397947)\n",
      "['–∏ —á–∏—Ç–∞—é—Ç—Å—è' '–æ–Ω –¥–µ–ª–∞–µ—Ç –Ω–µ' '–≤ –æ—Ç—á–µ—Ç–∏–∫–µ' 'recognize' '–≤–∞—Å –Ω–∞—Ä–∞—Å—Ç–∞–µ—Ç'\n",
      " '–º—ã—Å–ª—å –¥–∞ —á—Ç–æ' '–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∫–æ–Ω—Ñ—É—Ü–∏–π' '6000 —Ä—É–±–ª–µ–π' '–æ—Å—Ç–∞–≤–ª—è—é'\n",
      " '—Å–∏–º–ø–∞—Ç–∏—á–µ–Ω –Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ' '–∏ –ø–µ—Å—Ç—Ä–µ—é—Ç' '—Å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å—é –Ω–µ–ª—å–∑—è'\n",
      " '–ø—Ä–∞–≤–¥–∞ —Ç–æ–ª—å–∫–æ –≤' '–ø—Ä–æ—à–µ–ª –∑–æ–¥–∏–∞–∫–∞–ª—å–Ω—ã–π' '—á—Ç–æ –∑–∞–º–µ—Ä–∑—à–∞—è —Ä–µ–∫–∞'\n",
      " '–Ω–µ–ø–æ–≤—Ç–æ—Ä–∏–º—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥—ç–π–≤–∞' '–ª—é–¥–º–∏–ª–∞ –æ–≥–æ—Ä–æ–¥–æ–≤–∞' '—Ç–æ—á–Ω–æ –±—É–¥–µ—Ç'\n",
      " '–∏–∑–≤–∏–Ω—è—é—Å—å –Ω–æ –¥–∞–ª–µ–µ' '–≤—Å—Ç—Ä–µ—Ç–∏—à—å –≤']\n",
      "\n",
      "Accuracy on training sample: 0.66%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.66      0.75      0.70      1699\n",
      "       high       0.83      0.07      0.12       662\n",
      "        low       0.65      0.79      0.71      1717\n",
      "\n",
      "avg / total       0.68      0.66      0.61      4078\n",
      "\n",
      "Accuracy on test sample: 0.59%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.63      0.71      0.66       204\n",
      "       high       0.50      0.04      0.07        74\n",
      "        low       0.56      0.69      0.62       176\n",
      "\n",
      "avg / total       0.58      0.59      0.55       454\n",
      "\n",
      "\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for trait in ['HEX1_eX_nom', 'HEX2_A_nom', 'HEX3_C_nom', 'HEX4_E_nom', 'HEX5_O_nom', 'HEX6_H_nom']:\n",
    "    lm = RandomForestClassifier(n_estimators=500, max_features='log2', \n",
    "                                min_samples_leaf=20, oob_score = True)  \n",
    "    lm = LogisticRegression()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                         analyzer='word', \n",
    "                         tokenizer=word_tokenize, \n",
    "                         min_df = 30, \n",
    "                         max_df = 0.3, \n",
    "                         max_features = 10000)\n",
    "    build_model2(data3, X='text', y=trait, vectorizer=vectorizer, model=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Naive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we achieved anything at all? Are our models even \"napolshischechki\" better than naive model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NAIVE MODEL FOR HEX1_eX_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.35%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.39      0.42      0.40      1699\n",
      "       high       0.39      0.32      0.35      1704\n",
      "        low       0.25      0.30      0.28      1129\n",
      "\n",
      "avg / total       0.36      0.35      0.35      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX2_A_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.35%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.40      0.40      0.40      1781\n",
      "       high       0.29      0.32      0.31      1273\n",
      "        low       0.34      0.30      0.32      1478\n",
      "\n",
      "avg / total       0.35      0.35      0.35      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX3_C_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.26      0.41      0.32      1155\n",
      "       high       0.40      0.32      0.35      1760\n",
      "        low       0.36      0.30      0.33      1617\n",
      "\n",
      "avg / total       0.35      0.33      0.34      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX4_E_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.32%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.28      0.37      0.32      1356\n",
      "       high       0.48      0.30      0.37      2229\n",
      "        low       0.20      0.29      0.24       947\n",
      "\n",
      "avg / total       0.36      0.32      0.33      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX5_O_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.26      0.41      0.32      1137\n",
      "       high       0.48      0.31      0.38      2173\n",
      "        low       0.27      0.29      0.28      1222\n",
      "\n",
      "avg / total       0.37      0.33      0.34      4532\n",
      "\n",
      "================================================================================\n",
      "NAIVE MODEL FOR HEX6_H_nom\n",
      "================================================================================\n",
      "\n",
      "Accuracy of naive: 0.33%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    average       0.41      0.39      0.40      1903\n",
      "       high       0.16      0.30      0.21       736\n",
      "        low       0.39      0.28      0.33      1893\n",
      "\n",
      "avg / total       0.36      0.33      0.34      4532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_naive = np.random.choice(['high', 'average', 'low'], size=(len(data3),), p=[0.3, 0.4, 0.3])\n",
    "for trait in ['HEX1_eX_nom', 'HEX2_A_nom', 'HEX3_C_nom', 'HEX4_E_nom', 'HEX5_O_nom', 'HEX6_H_nom']:\n",
    "    print(\"=\"*80)\n",
    "    print('NAIVE MODEL FOR {}'.format(trait))\n",
    "    print(\"=\"*80)  \n",
    "    print('\\nAccuracy of naive: {:.2f}%'.format(accuracy_score(data[trait], y_naive)))\n",
    "    print(classification_report(data[trait], y_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems yes. Our model ~two times more precise than naive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
